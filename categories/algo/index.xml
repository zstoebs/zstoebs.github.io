<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/">
  <channel>
    <docs>https://blogs.law.harvard.edu/tech/rss</docs>
    <title>algo on Zach Stoebner | ml • neuro • kū</title>
    <link>/categories/algo/</link>
    <description>Recent content in algo on Zach Stoebner | ml • neuro • kū</description>
    <image>
      <title>algo on Zach Stoebner | ml • neuro • kū</title>
      <link>/categories/algo/</link>
      <url>https://source.unsplash.com/collection/983219/2000x1322</url>
    </image>
    <ttl>1440</ttl>
    <generator>After Dark 9.2.3 (Hugo 0.81.0)</generator>
    <language>en-US</language>
    <copyright>Copyright &amp;copy; Zachary Stoebner. Licensed under CC-BY-ND-4.0.</copyright>
    <lastBuildDate>Tue, 31 May 2022 22:09:32 UT</lastBuildDate>
    <atom:link href="/categories/algo/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>on linear complementarity problems</title>
      <link>/notes/lcp/</link>
      <pubDate>Wed, 24 Nov 2021 17:06:11 UT</pubDate>
      <dc:creator>Zach Stoebner</dc:creator>
      <guid>/notes/lcp/</guid>
      <description>This Fall 2021, I am taking a course on computational game theory, which insofar is the formulation of various games (e.g. bimatrix, Stackelberg) as mathematical programs and the algorithms that solve them, or approximate solutions. Linear complementarity problems are foundational for computing Nash equilibria of simple games.</description>
      <category domain="/categories/algo">Algo</category>
      <content:encoded><![CDATA[tl;dr This Fall 2021, I am taking a course on computational game theory, which insofar is the formulation of various games (e.g. bimatrix, Stackelberg) as mathematical programs and the algorithms that solve them, or approximate solutions. Linear complementarity problems are foundational for computing Nash equilibria of simple games.
Background Programming A mathematical program is an optimization (min, max) over an objective function and constraints. A linear program (LP) is one where the objective function and the constraints are all linear.
 The general LP formulation.  Aside: The terms &amp;lsquo;argmin&amp;rsquo; and &amp;lsquo;argmax&amp;rsquo; are special terminology for returning the optimizing value of the argument, instead of the optimal value of the function.
Complementarity A complementarity condition is a special kind of constraint required for solving linear complementarity problems (LCPs), as the name suggests. The non-negative vectors x and y are complements if one or both of the values at corresponding indices are 0.
 The definition of complementarity.  Complementarity results from a program&amp;rsquo;s transformation into an LCP. Generally, it is not a constraint defined in the programs of the game.
Games &amp;amp; Equilibria Intuitively, games are multiple programs that relate to each other. An equilibrium is a simultaneous joint solution that solves all optimization problems in a game.
In Nash games, it is assumed that the opponent will always play their optimal move, so the player should always play their optimal move. A Nash equilibrium is a best player&amp;rsquo;s best move without deviating from their predefined strategy, i.e., a matrix of costs for each of the player&amp;rsquo;s moves against each of the opponent&amp;rsquo;s moves. Solving Nash games is the same as finding the Nash equilibria.
Aside: John Nash proved that in every finite game all players can arrive at an optimal outcome.
Linear Complementarity Problem An LCP is defined as:
 The general form of an LCP.  LCPs are important and useful, for both mathematical and computer, programming because they can be analytically and algorithmically solved. Therefore, any programs that can be transformed into an LCP can be solved through the LCP; for Nash games, solutions to the LCP are thus the Nash equilibria.
Karush-Kuhn-Tucker Conditions The Karush-Kuhn-Tucker (KKT) conditions are a set of first-order conditions using a program&amp;rsquo;s objective function and constraints that must be satisfied by any optima. For the general form of a program:
 The general form of a mathematical program.  The KKT conditions are defined as:
 The KKT conditions for a general program.  The KKT conditions of the programs in a Nash game can be composed into an LCP. Solutions to the LCP satisfy the KKT conditions and are therefore Nash equilibria and solutions to the game.
For the linear program above, the KKT conditions are:
 The KKT conditions for a linear program.  which can then be composed into a function of the following form:
 The KKT conditions as a function for an LCP.  Complementarity conditions can then be enforced over this function of KKT conditions to form an LCP. Voilà!
Other programs, e.g., quadratic programs (QPs), can also be massaged into LCPs by stacking their KKT conditions in this way. Some problems can be massaged into complementarity problems, but not necessarily LCPs. Amusingly, the solutions to those problems are often approximated by solving LCPs.
Lemke&amp;rsquo;s Method Lemke&amp;rsquo;s method is a pivoting algorithm for computationally finding solutions to LCPs. The rearranged equation above can be organized into a tableau of the form:
 The tableau to solve the LCP via Lemke&#39;s method.  You may already see that essentially finding solutions to the LCP is solving a system of equations. However, since the tableau is a wide matrix, the system is underdetermined so there are infinitely many solutions, i.e., they lie in a conic region defined by complementary pairs of columns in the tableau, or no solutions, i.e., if q is not contained in any complementary cone.
For n variables in the z vector, there are necessarily 2n columns in the tableau with the n slack variables for w. Because of the stacked KKT conditions in the function, there are n rows. This suggests that any n of the variables, defined by their corresponding columns, can form a basis B to define the conic region containing a feasible solution. The decomposition of the system into basis and non-basis parts:
 The decomposition of an undetermined system into basis and non-basis components.  Since B is a basis of linearly independent columns, it can be inverted and the basis variables can technically determined that way. However, these matrices can be enormous in practice and inverting a matrix is very computationally expensive. Thankfully, iteratively pivoting the tableau is one way to accomplish the same goal without ever needing to invert a giant matrix. The high-level steps are:
 Use a minimum ratio test to determine the exiting variable from the basis. Row-reduce the tableau along the pivot column that corresponds to the entering variable at the row index of the exiting variable s.t. the pivot column is now one-hot at that row index. Replace the exiting variable with the entering variable at its index in the basis vector. The next entering variable is the complementary variable to the exiting variable, i.e., w_i -&amp;gt; z_i.  Stop conditions for the algorithm typically include: the values in the pivot columns are out of bounds, i.e., z_i &amp;lt; 0, or the initial entering variable z_0 leaves the basis. At the end of the algorithm, the values in the column corresponding to q will be the values of the basis variables in the final basis, and their complements will be 0. There are a number of modifications and tricks on top of this basic scaffolding that have been developed to handle variant complementarity problems. For discussion of each variant of Lemke&amp;rsquo;s method and their implementation details, I suggest reading Chapter 2 of the Murty textbook.
Other Complementarity Problems &amp;amp; Applications LCPs are in fact a very specific formulation of complementarity problems and generalizations exist, e.g., nonlinear (NCP), linear mixed (LMCP), and mixed (MCP) complementarity problems, which have looser constraints than an LCP. However, these problems typically cannot be analytically solved like LCPs, but can be approximated by LCPs.
Solving games underlies many practical applications, ranging from hot topics, such as autonomous driving and reinforcement learning, to age-old games, i.e., tag, motion planning, i.e., approximating MCPs with the PATH solver, and physics simulations.
  A simulation of a tag game. Converting a QP to an LCP and solving via Lemke&#39;s method are the crux of generating trajectories for each agent to optimally play the game.  References Murty. &amp;ldquo;LINEAR COMPLEMENTARITY, LINEAR AND NONLINEAR PROGRAMMING.&amp;rdquo; 1997.
Cottle, Pang, Stone. &amp;ldquo;The Linear Complementarity Problem.&amp;rdquo; 2008
Dirkse, Steven &amp;amp; Ferris, Michael. (1995). The path solver: a nommonotone stabilization scheme for mixed complementarity problems. Optimization Methods &amp;amp; Software - OPTIM METHOD SOFTW. 5. 123-156. 10.1080/10556789508805606.
GAMS PATH Solver
Nisan et al. &amp;ldquo;Algorithmic Game Theory.&amp;rdquo; 2007
Enzenhofer. &amp;ldquo;Numerical Solution of Mixed Linear Complementarity Problems in Multibody Dynamics with Contact.&amp;rdquo; 2018
Nocedal &amp;amp; Wright. &amp;ldquo;Numerical Optimization.&amp;rdquo; 2006.
Ralph. &amp;ldquo;Global Convergence of Damped Newton&amp;rsquo;s Method for Nonsmooth Equations via the Path Search.&amp;rdquo; 1990.
]]></content:encoded>
    </item>
    <item>
      <title>on fiber tracking</title>
      <link>/notes/fiber-tracking/</link>
      <pubDate>Tue, 16 Nov 2021 19:50:49 UT</pubDate>
      <dc:creator>Zach Stoebner</dc:creator>
      <guid>/notes/fiber-tracking/</guid>
      <description>After brief description of diffusion tensor images and what information they provide, I discuss an intuitive seed-based line propagation algorithm for computing a tractography map of a neuroimage. The open-source softwares required are 3D Slicer, ITK for C&#43;&#43;, and ITK-SNAP.</description>
      <category domain="/categories/algo">Algo</category>
      <content:encoded><![CDATA[tl;dr After brief description of diffusion tensor images and what information they provide, I discuss an intuitive seed-based line propagation algorithm for computing a tractography map of a neuroimage. A stack of open-source softwares that can be used to implement diffusion tensor tractography are 3D Slicer, ITK for C&#43;&#43;, and ITK-SNAP.
The diffusion tensor image used to generate all figures is publically available as sample data, DTIBrain, via 3D Slicer.
Diffusion Tensor Imaging In a diffusion tensor image (DTI), each voxel is represented by a symmetric tensor with 6 distinct values for the speed of water diffusion, corresponding to the directions from the center of the cubic voxel to each of its faces. Without obstruction, water diffusion is uniform on average in every direction; however, in spaces that alter diffusion, the values in each of these directions are anisotropic. Anisotropic diffusion gives rise to a principal axis of diffusion where some forward and backwards direction have greater values than the rest. Hence, the tensor at the voxel can be visualized as an ellipsoid – the elastic deformation of the cube along the principal axis.
 Sagittal slice of a DTI. 3D Slicer visualizes DTI well with a color orientation scheme, where red means that the principal eigenvector is along the left-right axis, blue means its along the inferior-superior axis, and green means its along the anterior-posterior axis. Intermediate colors means that its along a linear combination of these basis vectors.  Principal Eigenvector The major eigenvector from each diffusion tensor gives the forward direction of the principal axis; the negation of the major eigenvector is the backwards direction. Precomputing this vector image, with one vector at each voxel, yields a map that can be used to route the line propagation algorithm through the brain&amp;rsquo;s tracts. ITK defines a DiffusionTensor3D type that can set as the pixel type for an image; the type also defines a function for computing its eigendecomposition, which can be called when the pixel is accessed by an iterator.
 Sagittal slice of the principal eigenvector image of the DTI. Although it&#39;s noisy, you can see that there is still structure to it that makes sense, i.e., in the corpus callosum and the brain stem.  Fractional Anisotropy Fractional anisotropy (FA) is a scalar value computed from the eigenvalues of the diffusion tensor to quantify the amount of anisotropic diffusion at a voxel in the DTI [3]. A high value insinuates that water diffuses along a single axis, which would correspond to water flowing through a tract. FA for a 3x3 diffusion tensor is computed as follows:
 The formula to compute the scalar FA value, which is solely based on the eigenvalues, where the capped lambda is the mean of the eigenvalues.  When performing tractography, an FA value that is too low implies that water diffusion is largely isotropic, e.g., in grey matter, where there are no tracts. Hence, a minimum FA can be set and used as a stopping condition at a given voxel. ITK&amp;rsquo;s DiffusionTensor3D type defines a function to compute the FA value of the tensor and can be accessed in the same way as the eigendecomposition function.
 Sagittal slice of the FA image of the DTI. The FA image looks strikingly similar to a structural MRI.  Tractography With the directional values, a line can be propagated from a seed voxel along the 3D vector field to reconstruct a 3D tract down the two directions of the principal axis [1]. By propagating a line from a seed, or from many seeds, a rudimentary tractography map of a DTI can be computed. Although line propagation is intuitive and relatively lightweight, it is worth noting that other fiber tracking techniques exist that are both more robust and more intensive [2].
The stopping conditions:
 Current iteration is greater than the max number of iterations chosen by the user. The candidate voxel is outside of the image. The FA at the candidate voxel is below the minimum FA acceptable chosen by the user. The candidate voxel has already been visited.  If no stopping condition was met, then the current voxel was visited, meaning:
 The candidate voxel in the tractography image was set to a non-negative integer. The eigenvector at the candidate voxel was extracted, multiplied by some step size ∆ chosen by the user, added and subtracted from the candidate voxel’s index, and then pushed to the back of the list. Finally, the candidate voxel was popped from the front of the list and the iteration was incremented.  For this task, line propagation can be implemented both iteratively or recursively because it is isomorphic to search algorithms, i.e., breadth-first and depth-first search. Switching between the two is pretty straightforward as the function definitions should not be very different. That said, the recursive algorithm is likely less bloated and more effective computationally, but might be much slower if you&amp;rsquo;re using ITK in Python rather than C&#43;&#43;.
 Views of a tractography image generated by a single seed (top) and multiple seeds from a segmentation image (bottom) for ∆ = 0.9, minimum FA at 0.1, and the maximum number of iterations at 10000. The orientation of the views is given on the top. As expected, multi-seed input generated a much more extensive tractography map.  References [1] Mori S, van Zijl PC. Fiber tracking: principles and strategies - a technical review. NMR Biomed. 2002 Nov-Dec;15(7-8):468-80. https://doi.org/10.1002/nbm.781. PMID: 12489096.
[2] Kleiser R, Staempfli P, Valavanis A, Boesiger P, Kollias S. Impact of fMRI- guided advanced DTI fiber tracking techniques on their clinical applica- tions in patients with brain tumors. Neuroradiology. 2010 Jan;52(1):37-46. https://doi.org/10.1007/s00234-009-0539-2. Epub 2009 May 29. PMID: 19479248.
[3] Basser PJ, Pierpaoli C. Microstructural and physiological features of tissues eluci- dated by quantitative-diffusion-tensor MRI. J Magn Reson B. 1996 Jun;111(3):209- 19. https://doi.org/10.1006/jmrb.1996.0086. PMID: 8661285.
]]></content:encoded>
    </item>
    <item>
      <title>on fast inverse square root</title>
      <link>/notes/fast-inv-sqrt/</link>
      <pubDate>Fri, 02 Jul 2021 03:46:27 UT</pubDate>
      <dc:creator>Zach Stoebner</dc:creator>
      <guid>/notes/fast-inv-sqrt/</guid>
      <description>I found it on YouTube and, as my friend Nolan reminded me the other night, this algorithm is not new. The fast inverse square root shook the nerd world with its implementation in Quake III (1999).</description>
      <category domain="/categories/algo">Algo</category>
      <content:encoded><![CDATA[float Q_rsqrt( float number ) { long i; float x2, y; const float threehalfs = 1.5F; x2 = number * 0.5F; y = number; i = * ( long * ) &amp;amp;y; // part 1 i = 0x5f3759df - ( i &amp;gt;&amp;gt; 1 ); // part 2 y = * ( float * ) &amp;amp;i; y = y * ( threehalfs - ( x2 * y * y ) ); // part 3 } I found it on YouTube and, as my friend Nolan reminded me the other night, this algorithm is not new. The fast inverse square root shook the nerd world with its implementation in Quake III (1999).
Notice that it doesn’t use any division operator which is naturally slow on a digital computer; this algorithm speeds up computation of the inverse square root by 3x compared to conventional division and square root operations. The reason for writing it in C is evident in the first part, although these tricks also have analogs in often in other languages such as C&#43;&#43;, Python, etc. and is thus implemented in those languages as well.
Part 1 It uses tricks inherent in the language to cast the address of float (aka a float pointer) to a long pointer. The importance of using a long is evinced in the second part because it allows for quick division by 2. Since standard binary numbers, unlike floats, are not disjointed numbers in base 2, a single right bit shift performs floor halving.
Part 2 This part is the real meat of the trickery in this clever solution. The second half I already explained in the first part but the first half involves rearranging the floating-point formula for a square root division to solve for scalar values. Recall that floating point numbers are comprised of a mantissa and an exponent. The actual digit number can then be composed of a formula of these two parts: (1 &#43; M/2^23)*2^(E - 127) . The exponent is shifted down by 127 in IEEE 754 in order to represent negative values so 2^4 is actually 2^(131-127) where 131 is the number passed in the exponent. Since the mantissa needs to be between 1 and 2 in binary a 1 is fixed as the first digit before the point and the mantissa is divided by 2^23 since the mantissa is represented by 23 bits. Taking the logarithm of the above formula results in: log(1&#43;M/2^23) &#43; log(2^(E - 127)) . The trick to the problem is that log(1&#43;x) ~= x &#43; mu for small numbers (e.g. fractions less than 1). So this simplifies to: M/2^23 &#43; mu &#43; E - 127. Rearranging results in: 1/2^23 * (M &#43; 2^23*E) &#43; mu - 127 which is much more useful because the bit representation of a floating point number (M &#43; 2^23*E) is included. Recall that the binary number for a floating number is 8 bits for E followed by 23 bits for M.
Now that the background is setup, we can continue calculating 1/sqrt(x), or actually 1/sqrt(y) in this case. With the rearrangements above, it becomes much easier to calculate the logarithm of a floating point number. So log(1/sqrt(y)) = log(y^(-1/2)) = -(1/2)*log(y) . So if we want to solve for some G s.t. G = 1/sqrt(y) where log(G) = -(1/2)log(y), we can substitute in the above rearranged logarithm formula for a floating point number and solve for the bit representation for G which ends up as: M_G &#43; 2^23*E_G = (3/2)*2^23*(127 - mu) - (1/2)*(M_y &#43; 2^23*E_y). Hence, the first term is that bizarre hexadecimal number and the dexter term is the subtracted single rightshift to halve the number.
The reason for casting the floating point y to a long should be fairly obvious; we need the hard-coded bit representation of y to be treated as such, not as a floating point number which has different arithmetic operators.
Part 3 The third step is a little esoteric but not as technical as the second step. As with everything on computers, the numbers computed prior to this step are just an approximation. Due to all of the assumptions previously made (e.g. that the halving above isn’t floored for an odd number, that our chosen mu is accurate, etc.), we need to correct by some amount. Cue the Newton Iteration, which computes the error of x from a root for the function y. For this problem, the function is f(y) = 1/y^2 - x which rearranges to y = 1/sqrt(x) when y is a root (e.g. f(y) = 0). We need to compute x_new = x - f(x)/f’(x). Breaking down the derivative of a function: the derivative is described by the tangent line and where it intercepts the x-axis, composing the triangle below:
 A useful reference for a function derivative.  Since the ratio of the shifts times the shift in x equals the shift in y, the shift in x can be solved for as the shift in y divided by the ratio which ends up as: f(x)/f’(x). The actual implementation details of this aren’t made clear by the video, nor is the function derivative computed and I’m quite uncertain about which variable we take the derivative from, x or y. Nonetheless, it results in a static expression without any division operators required.
]]></content:encoded>
    </item>
  </channel>
</rss>
