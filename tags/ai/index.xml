<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/">
  <channel>
    <docs>https://blogs.law.harvard.edu/tech/rss</docs>
    <title>Ai on Zach Stoebner | ECE PhD @ UT Austin</title>
    <link>/tags/ai/</link>
    <description>Recent content in Ai on Zach Stoebner | ECE PhD @ UT Austin</description>
    <image>
      <title>Ai on Zach Stoebner | ECE PhD @ UT Austin</title>
      <link>/tags/ai/</link>
      <url>https://source.unsplash.com/collection/983219/2000x1322</url>
    </image>
    <ttl>1440</ttl>
    <generator>After Dark 9.2.3 (Hugo 0.148.2)</generator>
    <language>en-US</language>
    <copyright>Copyright &copy; Zachary Stoebner. Licensed under CC-BY-ND-4.0.</copyright>
    <lastBuildDate>Wed, 13 Aug 2025 00:15:01 UT</lastBuildDate>
    <atom:link href="/tags/ai/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>exploring compressed sensing fMRI time series</title>
      <link>/notes/csfmri-ts/</link>
      <pubDate>Tue, 31 May 2022 18:26:14 UT</pubDate>
      <dc:creator>Zach Stoebner</dc:creator>
      <guid>/notes/csfmri-ts/</guid>
      <description>An exploration of compressed sensing fMRI time series with 3 different algorithms. Typically, compressed sensing reconstructs a single volume of MRI but fMRI are composed of many volumes; sensing along the time domain could reduce the number of volumes required. Of the 3 algorithms, BSBL-BO performed the best with the error curve elbowing around 30% subsampling.</description>
      <category domain="/categories/course">Course</category>
      <category domain="/categories/solo">Solo</category>
      <content:encoded><![CDATA[ Signal sensing at 30% undersampling using the BSBL-BO algorithm. `$Y_i$` corresponds to time-domain signals whereas `$x_i$` corresponds to frequency-domain signals. tl;dr An exploration of compressed sensing fMRI time series with 3 different algorithms. Typically, compressed sensing reconstructs a single volume of MRI but fMRI are composed of many volumes; sensing along the time domain could reduce the number of volumes required. Of the 3 algorithms, BSBL-BO performed the best with the error curve elbowing around 30% subsampling.
Links repo
report
Abstract Compressed sensing reconstructs signals by solving underdetermined linear systems under the con- ditions that the measurements are sparse in the domain and incoherent [1]. In engineering, if measurements are taken within an appropriate basis satisfying the restricted isometry property, i.e., the Gaussian, Bernoulli, or Fourier bases, then this prior structure makes full signal recovery possible [2].
Compressed sensing is challenging with fMRI because the temporal dynamics of hemodynamic signals are relatively slow compared to other fast-acquisition signals that historically benefit from compressed sensing [3]. Additionally, having fewer samples insinuates a loss of statistical power in subsequent analyses. Thankfully, fMRI signals boast two beneficial characteristics that are promising for compressed sensing: 1. they are linear time-invariant, and 2. they lie within, and can transformed by, a Fourier basis [4].
In medical imaging, it is often assumed that an image is sampled at the Nyquist rate, s.t., enough discrete measurements are taken to reconstruct a continuous whole (M &gt; N) without loss of information. If high-fidelity reconstruction is possible sampling below the Nyquist rate, then MRI modalities would benefit since discerning a signal and quickly turning over an analysis reduces real costs. These potential gains beg the question: if an underdetermined linear system can be solved after sampling below the Nyquist rate, can we collect fewer samples and still recover a high-quality fMRI under a compressed sensing paradigm? The purpose of this project is to explore approaches to compressed sensing that yield meaningful signal recoveries from heavy undersampling.
Results Summary metrics of RMSE (left) and PSNR (right) voxel time series recovery using L1 minimization in a pure convex optimization formulation solved with the ECOS algorithm. Summary metrics of RMSE (left) and PSNR (right) voxel time series recovery using L1 minimization in a pure convex optimization formulation solved with the OWL-QN algorithm. Summary metrics of RMSE (left) and PSNR (right) voxel time series recovery using L1 minimization in a pure convex optimization formulation solved with the BSBL-BO algorithm. References [1] E. J. Candes et al., “Compressive sampling,” in Proceedings of the international congress of mathematicians, vol. 3, pp. 1433–1452, Citeseer, 2006.
[2] D. Angelosante, G. B. Giannakis, and E. Grossi, “Compressed sensing of time-varying signals,” in 2009 16th International Conference on Digital Signal Processing, pp. 1–8, IEEE, 2009.
[3] X. Zong, J. Lee, A. J. Poplawsky, S.-G. Kim, and J. C. Ye, “Compressed sensing fmri using gradient-recalled echo and epi sequences,” NeuroImage, vol. 92, pp. 312–321, 2014.
[4] O. Jeromin, M. S. Pattichis, and V. D. Calhoun, “Optimal compressed sensing reconstructions of fmri using 2d deterministic and stochastic sampling geometries,” Biomedical engineering online, vol. 11, no. 1, pp. 1–36, 2012.
[5] A. Domahidi, E. Chu, and S. Boyd, “ECOS: An SOCP solver for embedded systems,” in European Control Conference (ECC), pp. 3071–3076, 2013.
[6] G. Andrew and J. Gao, “Scalable training of l 1-regularized log-linear models,” in Proceedings of the 24th international conference on Machine learning, pp. 33–40, 2007.
[7] Z. Zhang and B. D. Rao, “Extension of sbl algorithms for the recovery of block sparse signals with intra-block correlation,” IEEE Transactions on Signal Processing, vol. 61, no. 8, pp. 2009– 2015, 2013.
[8] P. Wolfe, “Convergence conditions for ascent methods,” SIAM Review, vol. 11, no. 2, pp. 226– 235, 1969.
[9] H. Park and X. Liu, “Study on compressed sensing of action potential,” arXiv preprint arXiv:2102.00284, 2021.
[10] A. Jalal, M. Arvinte, G. Daras, E. Price, A. G. Dimakis, and J. Tamir, “Robust compressed sensing mri with deep generative priors,” Advances in Neural Information Processing Systems, vol. 34, pp. 14938–14954, 2021.
[11] X. Li, T. Cao, Y. Tong, X. Ma, Z. Niu, and H. Guo, “Deep residual network for highly accel- erated fmri reconstruction using variable density spiral trajectory,” Neurocomputing, vol. 398, pp. 338–346, 2020.
]]></content:encoded>
    </item>
    <item>
      <title>autonomous motion planning for an NVIDIA JetBot</title>
      <link>/notes/jetbot-motion-planning/</link>
      <pubDate>Thu, 09 Dec 2021 01:05:08 UT</pubDate>
      <dc:creator>Zach Stoebner</dc:creator>
      <guid>/notes/jetbot-motion-planning/</guid>
      <description>Built a JetBot + an exploration and novice implementation of motion planning on said JetBot. This computational game theory project marked my first foray into optimization and a glimpse of its power muahahaha. It ain&rsquo;t exquisite but it was heading in the right direction.</description>
      <category domain="/categories/course">Course</category>
      <category domain="/categories/solo">Solo</category>
      <content:encoded><![CDATA[ tl;dr Built a JetBot + an exploration and novice implementation of motion planning on said JetBot. This computational game theory project marked my first foray into optimization and a glimpse of its power muahahaha. It ain&rsquo;t exquisite but it was heading in the right direction.
Links repo
report
Background Although autonomous vehicles are the talk of the town these days, the methodologies behind them are elusive to most. To help yourself, I suggest reading my note on LCPs and then follow that up with section 3.1 of this thesis on LMCPs and the path solver for MCPs.
Path planning in autonomous vehicles is a booming research area with significant developments. Although computer vision and machine learning are often employed to plan motion in autonomous vehicles, computationally solving the optimization problems, that arise from in scenarios of motion planning, through a game theoretic is a lightweight alternative to path solving. In this scenario, the path planning optimization problem is formulated as a nonlinear complementarity problem (NCP) constrained by physics and simple car dynamics, which cannot necessarily be directly and exactly solved. Instead the NCP can be approximated by linear mixed complementarity problems (LMCPs), iteratively computing partial paths that together approximate the solution to the NCP and yield a motion planned trajectory for an autonomous vehicle.
The problem formulation is a non-visual scenario where stationary obstacles are laid out on a grid, in a predetermined fashion, and an optimal path must be computed through these obstacles to some goal point without exceeding bounds. Such paths are often nonlinear and can be closely approximated by solving linear mixed complementarity problems via a pathsolver algorithm. Once the path is determined, the JetBot must then move in a real setting, of which the software representation of the grid space is a projection.
Building the JetBot The JetBot was built following the documentation on the JetBot homepage. For the parts with multiple options: the IMX219-160 listed as the second option for cameras, the M2 card + antennas listed as the first option for wifi, and the 65mm wheels listed as the second option for wheels were used. The total cost was approximately $300. The hardware setup time was approximately twelve hours spread between two days. A significant portion of the time was spent extracting a screw terminal from the motor board that was placed incorrectly. 1 shows the completed JetBot hardware assembly.
Motion Path Solver This notebook contains code for non-visual motion planning – the primary objective of the project. The code relies on an LMCP solver that takes an LMCP formulation in M, q, l, u, x 0 and returns a path of points of z, w, v, t. A pathsolver then iteratively solves LMCPs for Newton points along an overarching path, performing backward linesearch to progress sufficiently down each of these paths towards the predefined goal point.
Solving many LMCPs approximates a nonlinear path, which can be formulated as an NMCP for which the KKT conditions must first be derived. The KKT conditions are formulated symbolically so that KKT function as well as the Jacobian of the KKT can be passed to the pathsolver for sparse JIT evaluation, accelerating runtime. Once the point sequence is acquired, it is passed to a module for JetBot motion planning to attempt to move the JetBot along the equivalent trajectory on a real grid space.
Motion Algorithms To achieve the best motion possible on the JetBot, various motion planning algorithms were implemented: linear approximation, Manhattan (aka wiggle) motion, and proportional / integrative / derivative (PID) control. For some of these algorithms, the arctan2 function is used to compute the angle for turning from one orientation to another. (The full code for the JetBotMotion class is included in the appendix.)
arctan 2(∆y, ∆x)
The approximate relevant specifications measured for the JetBot were:
With two obstacles, sometimes the pathsolver fails if dt is too small =⇒ dt &gt; 0.1
Confirmed that the solved states [x, y, v x , v y ] closely approximate the dynamics of horizontal motion
JetBot moves forward 40cm in 0.75 sec at speed=1
JetBot rotates 360 degrees in 1 sec at speed=1
Results Figure 1. Fully assembled JetBot. The two views show the camera, ports, wheels, and overall build structure of the JetBot. In the background is the soldering iron used during assembly. Figure 2. JetBot movement sequence for { (1, 1), (1, 0), (−1, 0), (−1, −1), (0, 1) } . Blue arrows indicate scene flow. The yellow arrow indicates the point that the JetBot should go to next; the yellow circle indicates the final location. Figure 3. Attempted JetBot linear approximation movement on a single obstacle course. The bottom left pane is the predicted trajectory from the pathsolver. Blue arrows indicate scene flow. The yellow arrow indicates the point that the JetBot should go to next; the yellow circle indicates the final location. Figure 4. Attempted JetBot linear approximation movement on a double obstacle course. The bottom left pane is the predicted trajectory from the pathsolver. Blue arrows indicate scene flow. The yellow arrow indicates the point that the JetBot should go to next; the yellow circle indicates the final location. Figure 5. Attempted JetBot Manhattan movement on a double obstacle course. The bottom left pane is the predicted trajectory from the pathsolver. Blue arrows indicate scene flow. The yellow arrow indicates the point that the JetBot should go to next; the yellow circle indicates the final location. Figure 6. Attempted JetBot PID control movement on a double obstacle course. The middle pane is the predicted trajectory from the pathsolver. Blue arrows indicate scene flow. The yellow arrow indicates the point that the JetBot should go to next; the yellow circle indicates the final location. This method resulted in a more correct, but still erroneous, path realization. References Enzenhofer. &ldquo;Numerical Solution of Mixed Linear Complementarity Problems in Multibody Dynamics with Contact.&rdquo; 2018
Dirkse, Steven &amp; Ferris, Michael. (1995). The path solver: a nommonotone stabilization scheme for mixed complementarity problems. Optimization Methods &amp; Software - OPTIM METHOD SOFTW. 5. 123-156. 10.1080/10556789508805606.
[1] Pepy, R., Lambert, A., and Mounier, H., “Path planning using a dynamic vehicle model,” in [2006 2nd International Conference on Information Communication Technologies], 1, 781–786 (2006). [2] Choset, H., La Civita, M., and Park, J., “Path planning between two points for a robot experiencing local- ization error in known and unknown environments,” (11 1999). [3] Lozano-Perez, T., “A simple motion-planning algorithm for general robot manipulators,” IEEE Journal on Robotics and Automation 3(3), 224–238 (1987). [4] Yonetani, R., Taniai, T., Barekatain, M., Nishimura, M., and Kanezaki, A., “Path planning using neural a* search,” in [International Conference on Machine Learning], 12029–12039, PMLR (2021). [5] Lee, L., Parisotto, E., Chaplot, D. S., Xing, E., and Salakhutdinov, R., “Gated path planning networks,” in [International Conference on Machine Learning], 2947–2955, PMLR (2018). [6] Mansouri, S. S., Kanellakis, C., Fresk, E., Kominiak, D., and Nikolakopoulos, G., “Cooperative coverage path planning for visual inspection,” Control Engineering Practice 74, 118–131 (2018). [7] Dirkse, S. and Ferris, M., “The path solver: A non-monotone stabilization scheme for mixed complementarity problems,” Optimization Methods and Software 5 (12 1993). [8] Andersson, J. A. E., Gillis, J., Horn, G., Rawlings, J. B., and Diehl, M., “CasADi – A software framework for nonlinear optimization and optimal control,” Mathematical Programming Computation (In Press, 2018). [9] Araki, M., “Pid control,” CONTROL SYSTEMS, ROBOTICS, AND AUTOMATION 2.
]]></content:encoded>
    </item>
    <item>
      <title>on linear complementarity problems</title>
      <link>/notes/lcp/</link>
      <pubDate>Wed, 24 Nov 2021 17:06:11 UT</pubDate>
      <dc:creator>Zach Stoebner</dc:creator>
      <guid>/notes/lcp/</guid>
      <description>This Fall 2021, I am taking a course on computational game theory, which insofar is the formulation of various games (e.g. bimatrix, Stackelberg) as mathematical programs and the algorithms that solve them, or approximate solutions. Linear complementarity problems are foundational for computing Nash equilibria of simple games.</description>
      <category domain="/categories/algo">Algo</category>
      <content:encoded><![CDATA[tl;dr This Fall 2021, I am taking a course on computational game theory, which insofar is the formulation of various games (e.g. bimatrix, Stackelberg) as mathematical programs and the algorithms that solve them, or approximate solutions. Linear complementarity problems are foundational for computing Nash equilibria of simple games.
Background Programming A mathematical program is an optimization (min, max) over an objective function and constraints. A linear program (LP) is one where the objective function and the constraints are all linear.
The general LP formulation. Aside: The terms &lsquo;argmin&rsquo; and &lsquo;argmax&rsquo; are special terminology for returning the optimizing value of the argument, instead of the optimal value of the function.
Complementarity A complementarity condition is a special kind of constraint required for solving linear complementarity problems (LCPs), as the name suggests. The non-negative vectors x and y are complements if one or both of the values at corresponding indices are 0.
The definition of complementarity. Complementarity results from a program&rsquo;s transformation into an LCP. Generally, it is not a constraint defined in the programs of the game.
Games &amp; Equilibria Intuitively, games are multiple programs that relate to each other. An equilibrium is a simultaneous joint solution that solves all optimization problems in a game.
In Nash games, it is assumed that the opponent will always play their optimal move, so the player should always play their optimal move. A Nash equilibrium is a best player&rsquo;s best move without deviating from their predefined strategy, i.e., a matrix of costs for each of the player&rsquo;s moves against each of the opponent&rsquo;s moves. Solving Nash games is the same as finding the Nash equilibria.
Aside: John Nash proved that in every finite game all players can arrive at an optimal outcome.
Linear Complementarity Problem An LCP is defined as:
The general form of an LCP. LCPs are important and useful, for both mathematical and computer, programming because they can be analytically and algorithmically solved. Therefore, any programs that can be transformed into an LCP can be solved through the LCP; for Nash games, solutions to the LCP are thus the Nash equilibria.
Karush-Kuhn-Tucker Conditions The Karush-Kuhn-Tucker (KKT) conditions are a set of first-order conditions using a program&rsquo;s objective function and constraints that must be satisfied by any optima. For the general form of a program:
The general form of a mathematical program. The KKT conditions are defined as:
The KKT conditions for a general program. The KKT conditions of the programs in a Nash game can be composed into an LCP. Solutions to the LCP satisfy the KKT conditions and are therefore Nash equilibria and solutions to the game.
For the linear program above, the KKT conditions are:
The KKT conditions for a linear program. which can then be composed into a function of the following form:
The KKT conditions as a function for an LCP. Complementarity conditions can then be enforced over this function of KKT conditions to form an LCP. Voilà!
Other programs, e.g., quadratic programs (QPs), can also be massaged into LCPs by stacking their KKT conditions in this way. Some problems can be massaged into complementarity problems, but not necessarily LCPs. Amusingly, the solutions to those problems are often approximated by solving LCPs.
Lemke&rsquo;s Method Lemke&rsquo;s method is a pivoting algorithm for computationally finding solutions to LCPs. The rearranged equation above can be organized into a tableau of the form:
The tableau to solve the LCP via Lemke's method. You may already see that essentially finding solutions to the LCP is solving a system of equations. However, since the tableau is a wide matrix, the system is underdetermined so there are infinitely many solutions, i.e., they lie in a conic region defined by complementary pairs of columns in the tableau, or no solutions, i.e., if q is not contained in any complementary cone.
For n variables in the z vector, there are necessarily 2n columns in the tableau with the n slack variables for w. Because of the stacked KKT conditions in the function, there are n rows. This suggests that any n of the variables, defined by their corresponding columns, can form a basis B to define the conic region containing a feasible solution. The decomposition of the system into basis and non-basis parts:
The decomposition of an undetermined system into basis and non-basis components. Since B is a basis of linearly independent columns, it can be inverted and the basis variables can technically determined that way. However, these matrices can be enormous in practice and inverting a matrix is very computationally expensive. Thankfully, iteratively pivoting the tableau is one way to accomplish the same goal without ever needing to invert a giant matrix. The high-level steps are:
Use a minimum ratio test to determine the exiting variable from the basis. Row-reduce the tableau along the pivot column that corresponds to the entering variable at the row index of the exiting variable s.t. the pivot column is now one-hot at that row index. Replace the exiting variable with the entering variable at its index in the basis vector. The next entering variable is the complementary variable to the exiting variable, i.e., w_i -&gt; z_i. Stop conditions for the algorithm typically include: the values in the pivot columns are out of bounds, i.e., z_i &lt; 0, or the initial entering variable z_0 leaves the basis. At the end of the algorithm, the values in the column corresponding to q will be the values of the basis variables in the final basis, and their complements will be 0. There are a number of modifications and tricks on top of this basic scaffolding that have been developed to handle variant complementarity problems. For discussion of each variant of Lemke&rsquo;s method and their implementation details, I suggest reading Chapter 2 of the Murty textbook.
Other Complementarity Problems &amp; Applications LCPs are in fact a very specific formulation of complementarity problems and generalizations exist, e.g., nonlinear (NCP), linear mixed (LMCP), and mixed (MCP) complementarity problems, which have looser constraints than an LCP. However, these problems typically cannot be analytically solved like LCPs, but can be approximated by LCPs.
Solving games underlies many practical applications, ranging from hot topics, such as autonomous driving and reinforcement learning, to age-old games, i.e., tag, motion planning, i.e., approximating MCPs with the PATH solver, and physics simulations.
A simulation of a tag game. Converting a QP to an LCP and solving via Lemke's method are the crux of generating trajectories for each agent to optimally play the game. References Murty. &ldquo;LINEAR COMPLEMENTARITY, LINEAR AND NONLINEAR PROGRAMMING.&rdquo; 1997.
Cottle, Pang, Stone. &ldquo;The Linear Complementarity Problem.&rdquo; 2008
Dirkse, Steven &amp; Ferris, Michael. (1995). The path solver: a nommonotone stabilization scheme for mixed complementarity problems. Optimization Methods &amp; Software - OPTIM METHOD SOFTW. 5. 123-156. 10.1080/10556789508805606.
GAMS PATH Solver
Nisan et al. &ldquo;Algorithmic Game Theory.&rdquo; 2007
Enzenhofer. &ldquo;Numerical Solution of Mixed Linear Complementarity Problems in Multibody Dynamics with Contact.&rdquo; 2018
Nocedal &amp; Wright. &ldquo;Numerical Optimization.&rdquo; 2006.
Ralph. &ldquo;Global Convergence of Damped Newton&rsquo;s Method for Nonsmooth Equations via the Path Search.&rdquo; 1990.
]]></content:encoded>
    </item>
    <item>
      <title>on fiber tracking</title>
      <link>/notes/fiber-tracking/</link>
      <pubDate>Tue, 16 Nov 2021 19:50:49 UT</pubDate>
      <dc:creator>Zach Stoebner</dc:creator>
      <guid>/notes/fiber-tracking/</guid>
      <description>After brief description of diffusion tensor images and what information they provide, I discuss an intuitive seed-based line propagation algorithm for computing a tractography map of a neuroimage. The open-source softwares required are 3D Slicer, ITK for C++, and ITK-SNAP.</description>
      <category domain="/categories/algo">Algo</category>
      <content:encoded><![CDATA[tl;dr After brief description of diffusion tensor images and what information they provide, I discuss an intuitive seed-based line propagation algorithm for computing a tractography map of a neuroimage. A stack of open-source softwares that can be used to implement diffusion tensor tractography are 3D Slicer, ITK for C++, and ITK-SNAP.
The diffusion tensor image used to generate all figures is publically available as sample data, DTIBrain, via 3D Slicer.
Diffusion Tensor Imaging In a diffusion tensor image (DTI), each voxel is represented by a symmetric tensor with 6 distinct values for the speed of water diffusion, corresponding to the directions from the center of the cubic voxel to each of its faces. Without obstruction, water diffusion is uniform on average in every direction; however, in spaces that alter diffusion, the values in each of these directions are anisotropic. Anisotropic diffusion gives rise to a principal axis of diffusion where some forward and backwards direction have greater values than the rest. Hence, the tensor at the voxel can be visualized as an ellipsoid – the elastic deformation of the cube along the principal axis.
Sagittal slice of a DTI. 3D Slicer visualizes DTI well with a color orientation scheme, where red means that the principal eigenvector is along the left-right axis, blue means its along the inferior-superior axis, and green means its along the anterior-posterior axis. Intermediate colors means that its along a linear combination of these basis vectors. Principal Eigenvector The major eigenvector from each diffusion tensor gives the forward direction of the principal axis; the negation of the major eigenvector is the backwards direction. Precomputing this vector image, with one vector at each voxel, yields a map that can be used to route the line propagation algorithm through the brain&rsquo;s tracts. ITK defines a DiffusionTensor3D type that can set as the pixel type for an image; the type also defines a function for computing its eigendecomposition, which can be called when the pixel is accessed by an iterator.
Sagittal slice of the principal eigenvector image of the DTI. Although it's noisy, you can see that there is still structure to it that makes sense, i.e., in the corpus callosum and the brain stem. Fractional Anisotropy Fractional anisotropy (FA) is a scalar value computed from the eigenvalues of the diffusion tensor to quantify the amount of anisotropic diffusion at a voxel in the DTI [3]. A high value insinuates that water diffuses along a single axis, which would correspond to water flowing through a tract. FA for a 3x3 diffusion tensor is computed as follows:
The formula to compute the scalar FA value, which is solely based on the eigenvalues, where the capped lambda is the mean of the eigenvalues. When performing tractography, an FA value that is too low implies that water diffusion is largely isotropic, e.g., in grey matter, where there are no tracts. Hence, a minimum FA can be set and used as a stopping condition at a given voxel. ITK&rsquo;s DiffusionTensor3D type defines a function to compute the FA value of the tensor and can be accessed in the same way as the eigendecomposition function.
Sagittal slice of the FA image of the DTI. The FA image looks strikingly similar to a structural MRI. Tractography With the directional values, a line can be propagated from a seed voxel along the 3D vector field to reconstruct a 3D tract down the two directions of the principal axis [1]. By propagating a line from a seed, or from many seeds, a rudimentary tractography map of a DTI can be computed. Although line propagation is intuitive and relatively lightweight, it is worth noting that other fiber tracking techniques exist that are both more robust and more intensive [2].
The stopping conditions:
Current iteration is greater than the max number of iterations chosen by the user. The candidate voxel is outside of the image. The FA at the candidate voxel is below the minimum FA acceptable chosen by the user. The candidate voxel has already been visited. If no stopping condition was met, then the current voxel was visited, meaning:
The candidate voxel in the tractography image was set to a non-negative integer. The eigenvector at the candidate voxel was extracted, multiplied by some step size ∆ chosen by the user, added and subtracted from the candidate voxel’s index, and then pushed to the back of the list. Finally, the candidate voxel was popped from the front of the list and the iteration was incremented. For this task, line propagation can be implemented both iteratively or recursively because it is isomorphic to search algorithms, i.e., breadth-first and depth-first search. Switching between the two is pretty straightforward as the function definitions should not be very different. That said, the recursive algorithm is likely less bloated and more effective computationally, but might be much slower if you&rsquo;re using ITK in Python rather than C++.
Views of a tractography image generated by a single seed (top) and multiple seeds from a segmentation image (bottom) for ∆ = 0.9, minimum FA at 0.1, and the maximum number of iterations at 10000. The orientation of the views is given on the top. As expected, multi-seed input generated a much more extensive tractography map. References [1] Mori S, van Zijl PC. Fiber tracking: principles and strategies - a technical review. NMR Biomed. 2002 Nov-Dec;15(7-8):468-80. https://doi.org/10.1002/nbm.781. PMID: 12489096.
[2] Kleiser R, Staempfli P, Valavanis A, Boesiger P, Kollias S. Impact of fMRI- guided advanced DTI fiber tracking techniques on their clinical applica- tions in patients with brain tumors. Neuroradiology. 2010 Jan;52(1):37-46. https://doi.org/10.1007/s00234-009-0539-2. Epub 2009 May 29. PMID: 19479248.
[3] Basser PJ, Pierpaoli C. Microstructural and physiological features of tissues eluci- dated by quantitative-diffusion-tensor MRI. J Magn Reson B. 1996 Jun;111(3):209- 19. https://doi.org/10.1006/jmrb.1996.0086. PMID: 8661285.
]]></content:encoded>
    </item>
    <item>
      <title>segmentation of kidney stones in endoscopic video feeds</title>
      <link>/notes/stone-anno/</link>
      <pubDate>Mon, 08 Nov 2021 02:39:01 UT</pubDate>
      <dc:creator>Zach Stoebner</dc:creator>
      <guid>/notes/stone-anno/</guid>
      <description>StoneAnno is my first published first-authorship paper, presenting at SPIE 2022. With the long-term goal of fully-automated robotic endoscopic surgery, we built a dataset of endoscopic kidney stone removal videos and investigated U-Net, U-Net++, and DenseNet for the segmentation task. We found a U-Net++ model that consistently achieves &gt;0.9 Dice score, with low loss, and produces realistic, convincing segmentations. Moving forward, I am implementing our model on hardware for deployment in ORs, as a part of my master&rsquo;s thesis, and I helped Dr. Kavoussi submit an R21 grant in October 2021.</description>
      <category domain="/categories/research">Research</category>
      <content:encoded><![CDATA[tl;dr StoneAnno is my first published first-authorship paper, presenting at SPIE 2022. My advisor, Prof. Ipek Oguz, connected me and my friend, David Lu, with Dr. Nick Kavoussi at VUMC who wanted to bring surgical endoscopy out of the dark ages. With the long-term goal of fully-automated robotic endoscopic surgery, we built a dataset of endoscopic kidney stone removal videos and investigated U-Net, U-Net++, and DenseNet for the segmentation task. We found a U-Net++ model that consistently achieves &gt;0.9 Dice score, with low loss, and produces realistic, convincing segmentations. Moving forward, I am implementing our model on hardware for deployment in ORs, as a part of my master&rsquo;s thesis, and I helped Dr. Kavoussi submit an R21 grant in October 2021. In the early stages of the project, we were also accepted for a poster presentation at the 2021 Engineering &amp; Urology Society annual meeting.
Citation Zachary A. Stoebner, Daiwei Lu, Seok Hee Hong, Nicholas L. Kavoussi, and Ipek Oguz &ldquo;Segmentation of kidney stones in endoscopic video feeds&rdquo;, Proc. SPIE 12032, Medical Imaging 2022: Image Processing, 120323G (4 April 2022); https://doi.org/10.1117/12.2613274
[SPIE] [arXiv]
Abstract Image segmentation has been increasingly applied in medical settings as recent developments have skyrocketed the potential applications of deep learning. Urology, specifically, is one field of medicine that is primed for the adoption of a real-time image segmentation system with the long-term aim of automating endoscopic stone treatment. In this project, we explored supervised deep learning models to annotate kidney stones in surgical endoscopic video feeds. In this paper, we describe how we built a dataset from the raw videos and how we developed a pipeline to automate as much of the process as possible. For the segmentation task, we adapted and analyzed three baseline deep learning models – U-Net, U-Net++, and DenseNet – to predict annotations on the frames of the endoscopic videos with the highest accuracy above 90%. To show clinical potential for real-time use, we also confirmed that our best trained model can accurately annotate new videos at 30 frames per second. Our results demonstrate that the proposed method justifies continued development and study of image segmentation to annotate ureteroscopic video feeds.
Results Figure 1. A comparison of vanilla (V) and pretrained (P) model training accuracies. Vanilla U-Net and U-Net++ start at a lower accuracy of approximately 0.75 Dice and gradually converge to a Dice score of approximately 0.88. ImageNet- pretrained U-Net and U-Net++ start at approximately 0.85 Dice, the convergence of the vanilla versions, and converge to about 0.91 and 0.92 Dice, respectively. Figure 2. A comparison of our best models’ Dice score (left) and BCE loss (right) from training. Note that the y-axis is scaled to the range of values. DenseNet had the highest and most variant BCE loss, yet its outlier values are relatively low compared to those in other binary classification tasks as BCE does not have an upper bound. Figure 3. Sample frame from the side-by-side video reconstruction of the input, ground truth, automated prediction with U-Net++, and heat map (left to right). The heat map is the raw probability output per pixel whereas the predicted segmentation is the pixels with probabilities ≥ 0.5. The model was able to compute this output at 30 FPS. Table 1. Summary of the statistics gathered for each model. Non-parenthetical values are the average scores from all frames in the test set. The reported value in parentheses is the maximum value recorded from each baseline’s validation during training. The highest performances between models for each metric are denoted in bold. U-Net++ claimed the highest scores in each metric on the test set. U-Net had the same test Dice score as U-Net++ but lower scores in all other metrics. DenseNet had relatively poor performance for all metrics. Only the ROC curves from test set performances are included. Video 1. Example performance video of our best U-Net++ video on new data, not in train / val / test sets. The video shows a saline treatment and a stone that is being broken down with a laser. Detritus from the laser treatment is collecting to the left of the stone. This video is a typical example of what the model would see in a practical setting. Future To deploy our high-performing model for surgical use in operating rooms, we will extend our video processing script to receive “in-step” frame-by-frame video input from a video capture card connected via DVI/HDMI to the endoscopic hardware. Then, we will output the side-by-side frames, as in Figure 5, to an adjacent monitor to assist physicians in surgery. Developing such a system will allow us to investigate further goals, such as monocular depth prediction which might prove critical in automation.18
We will also investigate the application of temporal models for our system. Incorporating information from previous frames might allow for more consistent prediction between subsequent frames. In addition, such models account for memory of data from previous frames without the overhead of additional input dimensions suffered by our current fully convolutional models. For this task, we will incorporate self-attention modules into our U-Net and U-Net++ architectures, which has been shown to increase performance in video segmentation.19
In practice, the problem domain also requires the segmentation of kidney stones after they have been surgically broken down into smaller pieces. The current dataset has been developed to support the segmentation of only whole kidney stones. Further development will include expansion of another section of the dataset where, as a surgeon breaks stones apart, the debris fragments will still be labeled by our model. In Figure 5, our model already segments clumps of debris; however, our future goal is finer granularity via an instance segmentation method.20
Additionally, we plan to incorporate multi-class segmentation to also identify, for example, healthy vs. un- healthy tissue. Since the task performs well on stone segmentation, we hypothesize that we can utilize the same underlying architectures to adapt to multi-class segmentation and that the model will perform similarly well, with relatively few manually annotated examples.21
References [1] Minaee, S., Boykov, Y. Y., Porikli, F., Plaza, A. J., Kehtarnavaz, N., and Terzopoulos, D., “Image Seg- mentation Using Deep Learning: A Survey,” IEEE Transactions on Pattern Analysis and Machine Intelli- gence 8828(c), 1–20 (2021).
[2] Zaitoun, N. M. and Aqel, M. J., “Survey on Image Segmentation Techniques,” in [Procedia Computer Science], 65, 797–806, Elsevier (jan 2015).
[3] Badrinarayanan, V., Kendall, A., and Cipolla, R., “SegNet: A Deep Convolutional Encoder-Decoder Ar- chitecture for Image Segmentation,” IEEE Transactions on Pattern Analysis and Machine Intelligence 39, 2481–2495 (dec 2017).
[4] Ronneberger, O., Fischer, P., and Brox, T., “U-net: Convolutional networks for biomedical image segmen- tation,” in [Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)], 9351, 234–241, Springer International Publishing, Cham (2015).
[5] Chen, L. C., Papandreou, G., Schroff, F., and Adam, H., “Rethinking atrous convolution for semantic image segmentation,” (jun 2017).
[6] Simonyan, K. and Zisserman, A., “Very deep convolutional networks for large-scale image recognition,” in [3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings], International Conference on Learning Representations, ICLR (sep 2015).
[7] Zhou, Z., Siddiquee, M. M. R., Tajbakhsh, N., Liang, J., Rahman Siddiquee, M. M., Tajbakhsh, N., and Liang, J., “UNet++: Redesigning Skip Connections to Exploit Multiscale Features in Image Segmentation,” IEEE Transactions on Medical Imaging 39(6), 1856–1867 (2020).
[8] Safi, S., Thiessen, T., and Schmailzl, K. J., “Acceptance and Resistance of New Digital Technologies in Medicine: Qualitative Study,” JMIR Res Protoc 7, e11072 (Dec 2018).
[9] Nithya, A., Appathurai, A., Venkatadri, N., Ramji, D. R., and Anna Palagan, C., “Kidney disease detection and segmentation using artificial neural network and multi-kernel k-means clustering for ultrasound images,” Measurement: Journal of the International Measurement Confederation 149, 106952 (jan 2020).
[10] Viswanath, K. and Gunasundari, R., “Design and analysis performance of kidney stone detection from ultrasound image by level set segmentation and ANN classification,” in [2014 International Conference on Advances in Computing, Communications and Informatics (ICACCI)], IEEE (sep 2014).
[11] Jegou, S., Drozdzal, M., Vazquez, D., Romero, A., and Bengio, Y., “The One Hundred Layers Tiramisu: Fully Convolutional DenseNets for Semantic Segmentation,” IEEE CVPR Workshops , 1175–1183 (2017).
[12] Huang, G., Liu, Z., Van Der Maaten, L., and Weinberger, K. Q., “Densely connected convolutional net- works,” IEEE CVPR 2017-Janua, 2261–2269 (2017).
[13] Otsu, N., “A Threshold Selection Method from Gray-level Histograms,” IEEE Transactions on Systems, Man and Cybernetics 9(1), 62–66 (1979).
[14] He, K., Zhang, X., Ren, S., and Sun, J., “Deep residual learning for image recognition,” IEEE CVPR 2016- Decem, 770–778 (2016).
[15] Sørensen, T., “A method of establishing groups of equal amplitude in plant sociology based on similar- ity of species and its application to analyses of the vegetation on danish commons”,” Kongelige Danske Videnskabernes Selskab 5(4), 1–34 (1948).
[16] Iremashvili, V., Li, S., Penniston, K. L., Best, S. L., Hedican, S. P., and Nakada, S. Y., “Role of Residual Fragments on the Risk of Repeat Surgery after Flexible Ureteroscopy and Laser Lithotripsy: Single Center Study,” J Urol 201, 358–363 (02 2019).
[17] Ward, T. M., Mascagni, P., Ban, Y., Rosman, G., Padoy, N., Meireles, O., and Hashimoto, D. A., “Computer vision in surgery,” Surgery 169(5), 1253–1256 (2021).
[18] Fu, H., Gong, M., Wang, C., Batmanghelich, K., and Tao, D., “Deep ordinal regression network for monoc- ular depth estimation,” in [Proceedings of the IEEE conference on computer vision and pattern recognition], 2002–2011 (2018).
[19] Ji, G.-P., Chou, Y.-C., Fan, D.-P., Chen, G., Fu, H., Jha, D., and Shao, L., “Progressively normalized self-attention network for video polyp segmentation,” arXiv preprint arXiv:2105.08468 (2021).
[20] Zhou, Y., Onder, O. F., Dou, Q., Tsougenis, E., Chen, H., and Heng, P.-A., “Cia-net: Robust nuclei instance segmentation with contour-aware information aggregation,” in [International Conference on Information Processing in Medical Imaging], 682–693, Springer (2019).
[21] Kayalibay, B., Jensen, G., and van der Smagt, P., “Cnn-based segmentation of medical imaging data,” arXiv preprint arXiv:1701.03056 (2017).
]]></content:encoded>
    </item>
    <item>
      <title>cortical surface analysis in Huntington&#39;s disease using linear-mixed models</title>
      <link>/notes/cortical-surface-analysis/</link>
      <pubDate>Tue, 14 Sep 2021 15:38:19 UT</pubDate>
      <dc:creator>Zach Stoebner</dc:creator>
      <guid>/notes/cortical-surface-analysis/</guid>
      <description>Although it will be published after StoneAnno, this shape analysis is my first completed research project and technically my first first-authorship, submitted to Brain. I wrote code in R and MATLAB to fit LMMs to the cortical data from T1w MRI of HD patients and then performed statistical analyses on the results using SurfStat and random field theory. We found that, with a novel method for measuring gyrification, LGI uniquely detects changes in the insula.</description>
      <category domain="/categories/research">Research</category>
      <content:encoded><![CDATA[tl;dr Although it will be published after StoneAnno, this shape analysis is my first completed research project and technically my first first-authorship, published in Human Brain Mapping. I wrote code in R and MATLAB to fit LMMs to the cortical data from T1w MRI of HD patients and then performed statistical analyses on the results using SurfStat and random field theory. We found that, with a novel method for measuring gyrification, LGI uniquely detects changes in the insula. Of note, I learned that complicated statistical anlayses are uniquely challenging, that I love LMMs and RFT, and that they are too esoteric in the current day &ndash; let&rsquo;s make them more accessible!
Citation Stoebner, Zachary A., et al. &ldquo;Comprehensive shape analysis of the cortex in Huntington&rsquo;s disease.&rdquo; Human Brain Mapping (2022).
https://doi.org/10.1002/hbm.26125
Background LMMs
random field theory
LGI acquisition method
Abstract The striatum has traditionally been the focus of Huntington’s disease research due to the primary insult to this region and its central role in motor symptoms. Beyond the striatum, evidence of cortical alterations caused by Huntington’s disease has surfaced. However, findings are not coherent between studies which have used cortical thickness for Huntington’s disease since it is the well-established cortical metric of interest in other diseases. In this study, we propose a more comprehensive approach to cortical morphology in Huntington’s disease using cortical thickness, sulcal depth, and local gyrification index. Our results show consistency with prior findings in cortical thickness, including its limitations. Our comparison between cortical thickness and local gyrification index underscores the complementary nature of these two measures &ndash; cortical thickness detects changes in the sensorimotor and posterior areas while local gyrification index identifies insular differences. Since local gyrification index and cortical thickness measures detect changes in different regions, the two used in tandem could provide a clinically relevant measure of disease progression. Our findings suggest that differences in insular regions may correspond to earlier neurodegeneration and may provide a complementary cortical measure for detection of subtle early cortical changes due to Huntington’s disease.
Results CT Fig 1. Omnibus test results for CT showing the regions of significant contrast across all patients compared to controls. P-values were adjusted for FWER using random field theory (\alpha=0.01). SD Fig 2. Omnibus test results for SD showing the regions of significant contrast across all patients compared to controls. P-values were adjusted for FWER using random field theory (\alpha=0.01). LGI Fig 3. Omnibus test results for LGI showing the regions of significant contrast across all patients compared to controls. P-values were adjusted for FWER using random field theory (\alpha=0.01). Summary Table 1. Summary of Regions with Significant Changes Per Feature. The percentage of the structure with significant changes are reported, in terms of the number of vertices. Regions are color-coded according to cooccurrence in the three features. Red = regional changes were detected by all three features. Yellow = regional changes were detected by two of the features. Blue = regional changes were detected by one of the features. Takeaways A main takeaway was learning the scientific process in action and, most importantly, learning to work with more experienced researchers. I wrote the code and performed all of the analysis that produced our results. However, I did not develop the awesome acquisition method that generated our LGI data nor the statistical theory behind the analysis. Throughout the project, I have relied heavily on the expertise of my co-authors &ndash; all of whom have PhDs whereas I was an undergrad until recently. This first journey in research has been inspiring and indelible. I am beyond grateful for it!
References Walker FO. Huntington’s disease. Lancet. 2007;369(9557):218-228. doi:10.1016/S0140-6736(07)60111-1
Long JD, Lee JM, Aylward EH, et al. Genetic Modification of Huntington Disease Acts Early in the Prediagnosis Phase. Am J Hum Genet. 2018;103(3):349-357. doi:10.1016/j.ajhg.2018.07.017
Paulsen JS, Ph D, Long JD, et al. Prediction of manifest Huntington disease with clinical and imaging measures : A 12-year prospective observational study. 2015;13(12):1193-1201. doi:10.1016/S1474-4422(14)70238-8.Prediction
Ehrlich ME. Huntington’s Disease and the Striatal Medium Spiny Neuron: Cell- Autonomous and Non-Cell-Autonomous Mechanisms of Disease. Neurotherapeutics. 2012;9(2):270-284. doi:10.1007/s13311-012-0112-2
Hett K, Johnson H, Coupe P, Paulsen JS, Long JD, Oguz I. Tensor-Based Grading: A Novel Patch-Based Grading Approach for the Analysis of Deformation Fields in Huntington’s Disease. Proc - Int Symp Biomed Imaging. 2020;2020-April:1091-1095. doi:10.1109/ISBI45749.2020.9098692
Li H, Zhang H, Johnson H, Long J, Paulsen J, Oguz I. Longitudinal subcortical segmentation with deep learning. In: SPIE Medical Imaging 2021: Image Processing. International Society for Optics and Photonics; 2021:115960D. doi:https://doi.org/10.1117/12.2582340
Li H, Zhang H, Hu D, et al. Generalizing MRI subcortical segmentation to Neurodegeneration. In: MLCN Workshop, MICCAI. Springer, Cham; 2020:139-147. doi:https://doi.org/10.1007/978-3-030-66843-3_14
Li H, Zhang H, Johnson H, Long J, Paulsen J, Oguz I. MRI Subcortical Segmentation In Neurodegeneration with Cascaded 3D CNNs. In: SPIE Medical Imaging 2021: Image Processing. International Society for Optics and Photonics; 2021:115960W. doi:https://doi.org/10.1117/12.2582005
Paulsen JS, Nopoulos PC, Aylward E, et al. Striatal and white matter predictors of estimated diagnosis for Huntington disease. Brain Res Bull. 2010;82(3-4):201-207. doi:10.1016/j.brainresbull.2010.04.003
Hedreen JC, Peyser CE, Folstein SE, Ross CA. Neuronal loss in layers V and VI of cerebral cortex in Huntington’s disease. Neurosci Lett. 1991;133(2):257-261. doi:10.1016/0304-3940(91)90583-F
Rosas HD, Liu AK, Hersch S, et al. Regional and progressive thinning of the cortical ribbon in Huntington’s disease. Neurology. 2002;58(5):695-701. doi:10.1212/WNL.58.5.695
Nopoulos PC, Aylward EH, Ross CA, et al. Cerebral cortex structure in prodromal Huntington disease. Neurobiol Dis. 2010;40(3):544-554. doi:10.1016/j.nbd.2010.07.014
Tabrizi SJ, Langbehn DR, Leavitt BR, et al. Biological and clinical manifestations of Huntington’s disease in the longitudinal TRACK-HD study: cross-sectional analysis of baseline data Sarah. 2013;8(9):791-801. doi:10.1016/S1474-4422(09)70170- X.Biological
Fischl B, Dale AM. Measuring the thickness of the human cerebral cortex from magnetic resonance images. Proc Natl Acad Sci U S A. 2000;97(20):11050-11055. doi:10.1073/pnas.200033797
Lyu I, Kang H, Woodward ND, Landman BA. Sulcal depth-based cortical shape analysis in normal healthy control and schizophrenia groups. 2018;1057402(March 2018):1. doi:10.1117/12.2293275
Lyu I, Kim SH, Girault JB, Gilmore JH, Styner MA. A cortical shape-adaptive approach to local gyrification index. Med Image Anal. 2018;48:244-258. doi:10.1016/j.media.2018.06.009
Wu D, Faria A V., Younes L, Ross CA, Mori S, Miller MI. Whole-brain segmentation and change-point analysis of anatomical brain mri—application in premanifest huntington’s disease. J Vis Exp. 2018;2018(136):1-9. doi:10.3791/57256
Tan X, Ross CA, Miller MI, Tang X. CHANGEPOINT ANALYSIS OF PUTAMEN AND THALAMUS SUBREGIONS IN PREMANIFEST HUNTINGTON’S DISEASE. In: 2018 IEEE 15th International Symposium on Biomedical Imaging. ; 2018:531-535. doi:10.1109/ISBI.2018.8363632
Tang X, Ross CA, Johnson H, et al. Regional subcortical shape analysis in premanifest Huntington’s disease. Hum Brain Mapp. 2019;40(5):1419-1433. doi:10.1002/hbm.24456
Hong Y, O’Donnell LJ, Savadjiev P, et al. Genetic load determines atrophy in hand cortico-striatal pathways in presymptomatic Huntington’s disease. Hum Brain Mapp. 2018;39(10):3871-3883. doi:10.1002/hbm.24217
Paulsen JS, Langbehn DR, Stout JC, et al. Detection of Huntington’s disease decades before diagnosis: The Predict-HD study. J Neurol Neurosurg Psychiatry. 2008;79(8):874-880. doi:10.1136/jnnp.2007.128728
Zhang Y, Long JD, Mills JA, Warner JH, Lu W, Paulsen JS. Indexing disease progression at study entry with individuals at-risk for Huntington disease. Am J Med Genet Part B Neuropsychiatr Genet. 2011;156(7):751-763. doi:10.1002/ajmg.b.31232
Fischl B. FreeSurfer. Neuroimage. 2012;62(2):774-781. doi:10.1016/j.neuroimage.2012.01.021.FreeSurfer
Lyu I, Kang H, Woodward ND, Styner MA, Landman BA. Hierarchical spherical deformation for cortical surface registration. Med Image Anal. 2019;57:72-88. doi:10.1016/j.media.2019.06.013
Parvathaneni P, Bao S, Nath V, et al. Cortical Surface Parcellation Using Spherical Convolutional Neural Networks. Lect Notes Comput Sci (including Subser Lect Notes Artif Intell Lect Notes Bioinformatics). 2019;11766 LNCS:501-509. doi:10.1007/978- 3-030-32248-9_56
Klein A, Canton TD, Ghosh SS, et al. Open labels: online feedback for a public resource of manually labeled brain images. 16th Annu Meet Organ Hum Brain Mapping. Published online 2010:84358.
Moorhead TWJ, Harris JM, Stanfield AC, et al. Automated computation of the Gyrification Index in prefrontal lobes: Methods and comparison with manual implementation. Neuroimage. 2006;31(4):1560-1566. doi:10.1016/j.neuroimage.2006.02.025
Roberts M, Hanaway J, Morest DK. Atlas of the Human Brain in Section. 2nd ed. Lea &amp; Febiger; 1970.
Lyu I, Kim SH, Woodward ND, Styner MA, Landman BA. TRACE: A Topological Graph Representation for Automatic Sulcal Curve Extraction. IEEE Trans Med Imaging. 2018;37(7):1653-1663. doi:10.1109/TMI.2017.2787589
Alin A. Multicollinearity. Wiley Interdiscip Rev Comput Stat. 2010;2(3):370-374. doi:10.1002/wics.84
Verbeke G, Molenberghs G. Linear Mixed Models for Longitudinal Data. Print. Spinger; 2000. https://link.springer.com/chapter/10.1007/978-0-387-22775-7_3
Worsley KJ, Andermann M, Koulis T, MacDonald D, Evans AC. Detecting changes in nonisotropic images. Hum Brain Mapp. 1999;8(2-3):98-101. doi:10.1002/(SICI)1097- 0193(1999)8:2/3&lt;98::AID-HBM5&gt;3.0.CO;2-F
Taylor JE, Worsley KJ. Detecting sparse signals in random fields, with an application to brain mapping. J Am Stat Assoc. 2007;102(479):913-928. doi:10.1198/016214507000000815
Bates D, Mächler M, Bolker BM, Walker SC. Fitting linear mixed-effects models using lme4. J Stat Softw. 2015;67(1). doi:10.18637/jss.v067.i01
Worsley K, Taylor J, Carbonell F, et al. SurfStat: A Matlab toolbox for the statistical analysis of univariate and multivariate surface and volumetric data using linear mixed effects models and random field theory. Neuroimage. 2009;47:S102. doi:10.1016/s1053-8119(09)70882-1
Han X, Jovicich J, Salat D, et al. Reliability of MRI-derived measurements of human cerebral cortical thickness: The effects of field strength, scanner upgrade and manufacturer. Neuroimage. 2006;32(1):180-194. doi:10.1016/j.neuroimage.2006.02.051
Hong EP, MacDonald ME, Wheeler VC, et al. Huntington’s Disease Pathogenesis: Two Sequential Components. J Huntingtons Dis. 2021;10(1):35-51. doi:10.3233/JHD- 200427
Mangin JF, Rivière D, Duchesnay E, et al. Neocortical morphometry in Huntington’s disease: Indication of the coexistence of abnormal neurodevelopmental and neurodegenerative processes. NeuroImage Clin. 2020;26(February):102211. doi:10.1016/j.nicl.2020.102211
Scahill RI, Zeun P, Osborne-Crowley K, et al. Biological and clinical characteristics of gene carriers far from predicted onset in the Huntington’s disease Young Adult Study (HD-YAS): a cross-sectional analysis. Lancet Neurol. 2020;19(6):502-512. doi:10.1016/S1474-4422(20)30143-5
Nopoulos P, Magnotta VA, Ph D, et al. Morphology of the Cerebral Cortex in Preclinical Huntington’s Disease. Am J Psychiatry. 2012;164(September 2007):1428- 1434.
Brown K. Encyclopedia of Language and Linguistics. Vol 1. Elsevier; 2005.
]]></content:encoded>
    </item>
    <item>
      <title>verification of a VAE &amp; SegNet using NNV</title>
      <link>/notes/vae-segnet-verif/</link>
      <pubDate>Fri, 09 Jul 2021 18:51:01 UT</pubDate>
      <dc:creator>Zach Stoebner</dc:creator>
      <guid>/notes/vae-segnet-verif/</guid>
      <description>Neural network automated verification of a VAE and SegNet using NNV. Although neural networks are promising, they are easily confused, particularly if the input domain is perturbed. In this project, I demonstrate the robustness of MNIST-trained VAE and SegNet against varying brightness attacks.</description>
      <category domain="/categories/course">Course</category>
      <category domain="/categories/solo">Solo</category>
      <content:encoded><![CDATA[ Lower bound and upper bound attacks on a generated image from a VAE. tl;dr Neural network automated verification of a VAE and SegNet using NNV. Although neural networks are promising, they are easily confused, particularly if the input domain is perturbed. In this project, I demonstrate the robustness of MNIST-trained VAE and SegNet against varying brightness attacks.
Links repo
report
Motivation After many face palms and harrowing all-nighters, I learned that achieving reliable performance, for even a task as simple-sounding as face following, is a true feat worthy of ascension from padawan to knight. Even though teams of engineers can concoct relatively high-performing models, I&rsquo;ve learned from first-hand experience, as many others have, that even at the highest level ML messes up&hellip; a lot.
For computer vision in particular, neural networks [NNs] tend to mess up when images are perturbed in some way, e.g., a brightness attack or a deformation, which may be imperceptible to the human eye. Read about optical adversarial attacks against road sign classifiers and defeating deepfake detectors. Subtle intensity differences between images frequently confounds analysis in image-based learning and applications; for example, I also research methods for harmonization of longitudinal MRI datasets using GANs to correct these minute differences. As I was learning about formal methods and automated verification, I wondered whether NNs can be verified for robustness against attacks and, in turn, also verify the performance of generator networks, such as autoencoders, in generating utile images. As a learning experience, this exploratory project is a classic case of &ldquo;know thy enemy&rdquo;.
Content train_segnet.m: trains a SegNet on MNIST train_VAE.m: trains a VAE on MNIST verify_segnet.m: attempts verification of SegNet given perturbed or unperturbed input verify_VAE.m: attempts verification of VAE given perturbed or unperturbed input Other files are ported or modified from MATLAB&rsquo;s VAE demo.
Method For a light background on some of the set theory behind neural network verification and why this task is particularly challenging, please refer to the report linked at the top of this page.
For the first part of the project, I trained a VAE and SegNet, both with the ELBO loss function, on MNIST. Figures 1 and 2 display real and fake images for each network; the VAE tended to blur the image whereas the SegNet learned the structures suspiciously well, potentially a result of overfitting.
Figure 1. Examples of real inputs (left) and fake outputs (right) for the VAE. The pairs demonstrate examples where the VAE performs well and where it does not, noticeably a ‘9’ that becomes an ‘8’ and a ‘4’ that becomes a ‘9’. Figure 2. Examples of real inputs (left) and fake outputs (right) for the SegNet autoencoder. Compared to the VAE, the SegNet is less confused about the structure of the input images. However, the two channel output results in an average of the binary masks which gives a gray image. After training the generator networks, I parsed them into NNV format and appended an MNIST classifier to each of them. In a tutorial, NNV constructed a simple CNN MNIST classifier that performed exceedingly well so I ported that model into this project.
To verify robustness of the generator + classifier, the top 1% or 5% of pixels in input images were eliminated, where the lower bound zeroed out the original intensity while the upper bound kept 5% of the original intensity. From here, NNV constructs polytope set representations of the input domain to feed to the NNV representations of the networks, in order to approximate the entirety of the output domain as a zonotope, star, and abstract polytope reach set.
In terms of analysis, NNV plots error bars, denoting the upper and lower bound probabilities that a given example belongs to each class, for each reach set. A network is robust for a certain class, for a certain set, if the lower bound for the class is greater than the upper bound for all other classes, given an approximated reach set.
Results The approximated zonotope reach set was inconclusive for both models at each level of attack whereas ImageStar and abstract polytope sets had almost identical results and differentiated model performance; therefore, these two reach sets were used in subsequent analyses. For the baseline, the output domains became too varied with greater levels of attack that were not reasonably computable on a CPU so no data was gathered for classes at those levels of attack for the baseline. Table 1 contains a summary of the results for each class for MNIST (baseline) and generated VAE &amp; SegNet images. Both models were particularly robust to the digit 3; the star set results are displayed for this class are displayed in Figure 3.
Table 1. Summary of results per class for baseline, VAE, and SegNet images passed to the classifier. - = robust; {0,1,2,3,4,5,6,7,8,9} = not robust, misclassified to digit; ? = not robust, uncertain, overlapping ranges; ~ = no data. Figure 3. Error bars for the digit ‘3’ for each method for the ImageStar set for the 1% attack. The classifier was distinctly robust to this class for the three types of images, for both attacks. Compared to other classes, the error bars for the ‘3’ class are very narrow for an attacked image. Future This field of NN verification is extremely new and methods for verifying networks with activations other than ReLU and sigmoid are not well-defined. I learned about the field of neural network verification from this project and at the time of writing I am satisfied by what I&rsquo;ve learned. Extending the project, formulating a procedure for verifying generator network outputs with automated NN verification would be a useful tool for many an ML practicioner. A procedure with the current SOTA using my generator + classifier setup might be feasible; however, I expect that we will have to wait for the field to progress significantly further.
References X. Huang et al., “A Survey of Safety and Trustworthiness of Deep Neural Networks: Verification, Testing, Adversarial Attack and Defence, and Interpretability ∗,” arXiv, pp. 0–94, 2018.
C. Liu, T. Arnon, C. Lazarus, C. Barrett, and M. J. Kochenderfer, “Algorithms for verifying deep neural networks,” arXiv, pp. 1–126, 2019, doi: 10.1561/2400000035.
K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale image recognition,” 3rd Int. Conf. Learn. Represent. ICLR 2015 - Conf. Track Proc., pp. 1–14, 2015.
H. D. Tran et al., “NNV: The Neural Network Verification Tool for Deep Neural Networks and Learning-Enabled Cyber-Physical Systems,” Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics), vol. 12224 LNCS, pp. 3–17, 2020, doi: 10.1007/978-3- 030-53288-8_1.
Y. Lecun, L. Bottou, Y. Bengio, and P. Ha, “LeNet,” Proc. IEEE, no. November, pp. 1–46, 1998.
D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” 2nd Int. Conf. Learn. Represent. ICLR 2014 - Conf. Track Proc., no. Ml, pp. 1–14, 2014.
V. Badrinarayanan, A. Kendall, and R. Cipolla, “SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 39, no. 12, pp. 2481–2495, 2017, doi: 10.1109/TPAMI.2016.2644615.
A. A. Alemi, B. Poole, I. Fische, J. V. Dillon, R. A. Saurous, and K. Murphy, “Fixing a broken elbo,” 35th Int. Conf. Mach. Learn. ICML 2018, vol. 1, pp. 245–265, 2018.
H. D. Tran et al., “Star-based reachability analysis of deep neural networks,” Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics), vol. 11800 LNCS, pp. 670–686, 2019, doi: 10.1007/978-3-030-30942-8_39.
]]></content:encoded>
    </item>
    <item>
      <title>visualizing temporal graph networks</title>
      <link>/notes/tgn-viz/</link>
      <pubDate>Fri, 09 Jul 2021 18:28:40 UT</pubDate>
      <dc:creator>Zach Stoebner</dc:creator>
      <guid>/notes/tgn-viz/</guid>
      <description>Visualizing the resulting link prediction graph from a temporal graph network on a Wikipedia dataset using Observable and d3.js.</description>
      <category domain="/categories/course">Course</category>
      <category domain="/categories/team">Team</category>
      <content:encoded><![CDATA[tl;dr Visualizing the resulting link prediction graph from a temporal graph network on a Wikipedia dataset using Observable and d3.js.
Usage Clone the repo. cd viz/ python server.py Open the project notebook in Chrome. Safari will likely not work with the server. Tips:
If the view cells aren&rsquo;t rendered, run the server endpoint cells. It&rsquo;s a big, complex viz so it may take a while to compute the full graph to a point where you can interact with the views. Links Observable
GitHub
Motivation It seems like, in the past year, graph neural networks (GNN) have swept through every research circle and someone has spearheaded a journal club presentation about them. Anyways, my friend, Cole, was investigating a new GNN called a temporal graph network (TGN) for his masters thesis. To kill two birds with one stone, he proposed using a TGN for our course project in visual analytics &amp; ML and put into practice this esoteric method that we&rsquo;d only talked about so far.
For more background information, visit the Observable notebook linked above.
Method Preliminaries &amp; Back end: To generate the graph using ACCRE, we ran the TGN on the Wikipedia dataset from http://snap.stanford.edu/jodie/ on both the basic link prediction task for the probability data and the node classification task for the context data. To preprocess and arrange the graph, we used NetworkX. To serve the data, we used Flask and, to compute t-SNE, we used SciKit-Learn. All backend code was written in Python and can be cloned from https://github.com/zstoebs/tgn.
Front end: TGNVis currently has 3 main views: a full graph view, a subgraph view, and a subgraph scatterplot view. The full graph view consists of a link-node diagram that displays all of the nodes in the validation set. Each link color is scaled logarithmically based on the timestamp of the link occurring, with brighter and more saturated colors representing later timestamps. Link opacity linearly encodes the probability of the link occurring according to our model, with more opaque links signifying a higher predicted probability. Users can pan and zoom around the full graph view as well as brush when holding the Alt key (Windows / Linux) or Command key (macOS) to select a subset of nodes that will populate the subgraph view and the scatterplot. When nodes are brushed in the full graph, they will change colors from black to purple. Brushing in d3.js does not coexist well with most other interactions so it is inherently bugged; we suggest waiting for the force simulation to settle and trying to repeatedly to brush.
The subgraph view consists of nodes that have been brushed on the full graph view, plus all of their 1-hop connections. Once the subgraph is populated, hovering over a node or a link will provide the available information about the node or link being displayed. Once again, users can pan and zoom around the subgraph to inspect specific elements. Upon brushing again in the full graph, the subgraph and scatterplots will repopulate.
The scatterplot view contains the 2-component t-SNE dimensionality reduction view of the context embeddings available for the source and destination nodes of links on the subgraph. In order to retain the most information possible for each link, we provide two subplots that display reduced source and destination embeddings, left to right respectively. We also provide brushing functionality for our scatterplot view on the source node plot. When the plot is brushed, the brushed points will change color to blue, and the corresponding points in the destination scatterplot will also change color to red and lines are drawn to identify the links between selected nodes in the scatterplots. In addition, the subgraph view will update the colors of the selected nodes accordingly with one caveat: if it corresponds to both a source and destination embedding, it will change to purple. Nodes are often involved in multiple link events and therefore have multiple instances in
Views Full Graph Fig 1. The full-graph view displays the full link prediction graph output. It serves as the highest point for analysis. Users can zoom, pan, and brush over a subgraph for closer inspection. Subgraph Fig 2. The subgraph view displays the brushed selection from the full-graph view. Hovering over links and nodes populates their information. For nodes, hovering just displays the node's ID whereas hovering over the link populates source and target IDs, predicted probability, and the timestamp. Scatterplot Fig 3. The scatterplot view displays the 2-dim t-SNE reduction for all source and target nodes in the subgraph view. The view allows brushing over the source nodes to identify their corresponding target nodes and also highlight the selected source and target nodes accordingly in the subgraph. References F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini, “The graph neural network model,” IEEE Trans. Neural Networks, vol. 20, no. 1, pp. 61–80, 2009, doi: 10.1109/TNN.2008.2005605.
W. L. Hamilton, “Inductive Representation Learning on Large Graphs,” no. Nips, pp. 1–19, 2017.
E. Rossi, B. Chamberlain, F. Frasca, D. Eynard, F. Monti, and M. Bronstein, “Temporal Graph Networks for Deep Learning on Dynamic Graphs,” in ICML, 2020, pp. 1–16.
R. Ying, D. Bourgeois, J. You, M. Zitnik, and J. Leskovec, “GNNExplainer: Generating explanations for graph neural networks,” arXiv, no. iii, 2019.
Z. Jin, Y. Wang, Q. Wang, Y. Ming, T. Ma, and H. Qu, “GNNVis : A Visual Analytics Approach for Prediction Error Diagnosis of Graph Neural Networks,” vol. XX, no. Xx, pp. 1–14, 2020.
Michael Bostock&rsquo;s Temporal Force-Directed Graph
Scax&rsquo;s Force-Directed Graph with Zooming
]]></content:encoded>
    </item>
    <item>
      <title>face following and vSLAM for a Tello quadcopter</title>
      <link>/notes/tello-slam/</link>
      <pubDate>Fri, 02 Jul 2021 16:46:28 UT</pubDate>
      <dc:creator>Zach Stoebner</dc:creator>
      <guid>/notes/tello-slam/</guid>
      <description>Implementation of face detection / following and vSLAM on a Ryze Tello using its MATLAB toolkit.</description>
      <category domain="/categories/course">Course</category>
      <category domain="/categories/solo">Solo</category>
      <content:encoded><![CDATA[tl;dr Implementation of face detection / following and vSLAM on a Ryze Tello using its MATLAB toolkit.
Links repo
report
Motivation Following my quad build experience, I set the intention to continue working with and learning more about quads. Whereas in that project I focused more on the hardware side of quads, I wanted to focus more on the software side in this one. Specifically, I wanted to program a quad with autonomous functionality. While working on the quad build, I stumbled upon face detection &amp; following and SLAM. Face detection &amp; following is straightforward: use deep learning to draw a bounding box around faces in the image and compute the direction to travel based on the size and offset from the image&rsquo;s center. vSLAM on the other hand is more interesting in my opinion. For those that don&rsquo;t know: simultaneous localization &amp; mapping (SLAM) uses sensor data, i.e., lidar, radar, camera, etc., to create a map and track the location(s) of the agent(s) on the map. This problem is intractable and elegantly implementing it in the field is a unique challenge, often requiring a team with intimate knowledge of the UAV to tailor clever SLAM algorithms to it.
Contents main.m: control flow script to demonstrate each of these on the Tello. follow.m: face detection and following algorithm that returns the movement vector required to center the drone on the detected face, if there is one. vslam.m: implements vSLAM using the drone&rsquo;s pinhole camera given a predetermined movement sequence that should be cycled a handful of times. Method Face Following Algorithm:
Pass the frame to the object detector and retrieve a bounding box location(s) for the detected object. Draw boxes around all of the detected images. Use the closest bounding box’s width and center coordinates to compute the relative axis change as a percentage of the max. Based on some threshold percentage and some minimum movement distance, set the axes distances and return them to be used in a move command. Fig 1. Face following schematic. An image is passed to the cascade object detector. The detector draws a bounding box around the face. The centering vector from the current center to the center of the bounding box is computed. The UAV moves in the direction of the centering vector while maintaining a safe, specified distance. vSLAM This vSLAM implementation breaks down into three key parts: map initialization, tracking and local mapping.
Starting with map initialization, the steps are as follows:
Track the ORB features on the first image to load the pre-points, then track a second image. Match the ORB feature correspondences between the two images. If enough matches are made (100), compute the homography and fundamental matrices so that the correct geometric transform is applied based on which results in the least error for the relative camera pose. If insufficient matches are made, then the loop restarts on a new image. Manually, the loop has a maximum of 5 iterations to find a matched image until an error is thrown. If a match is not made in 5 iterations, it may imply that the Tello has weak connection and low light and needs to be reset. Triangulate the 3D locations of the matched features in the new map. For tracking:
Move the drone according to the modulus of the current move index by the length of the move sequence. If the Tello loses connection and throws an error, loop back to see if connection is regained, changing no indices except a break iteration countdown of 10. Throw the error if the break countdown expires. Extract ORB features from the frame and match with the latest keyframe. If the new frame is not a keyframe, continue the loop. Estimate the camera pose with Perspective-n-Point [10] in order to project the features to the current frames perspective and correct using some bundle adjustments[8]. This step, although esoteric, is important for the fast computation of that ORB-SLAM offers compared to the competition. Determine if the current frame is a key frame given the criteria. If so, the process continues to local mapping. Else, the loop iterates, and the above steps are redone for the next frame. Additionally, this step also speeds up the process; instead of evaluating all of the features in every frame for mapping, only a select few that are substantially different are filtered for usage. The local mapping steps are as follows:
Add the new keyframe to the set. Compare the keyframes features against all the other keyframe features, looking for unmatched points that occur in at least 3 other keyframes. Bundle adjust the pose based on the adjacent keyframes’ poses. Fig 2. Visual ORB-SLAM schematic. The process starts by initializing the map with two initial frames from the camera. During the initialization the UAV jiggles up and down to snapshot slightly different pairs of images with different feature extractions but still with some matches. If the map initializes, then the program proceeds to the main loop where it first tracks the features on a new frame. If the frame is a keyframe, then the new features are updated into the map. If the frame is not a keyframe, then the loop continues. At the start of each loop iteration, the UAV executes the next move in the sequence. Results Face Following Fig 3. Examples of when my face is detected. Looking at the Tello (left) and not looking at the Tello (right). Nonetheless, it still detects my face and doesn’t pick up much noise, even in low light. Fig 4. Example of face misclassification. These misclassifications typically occur when there is no face in view of the camera. Otherwise, they are rare and not noticeable during a face following run. Face following was easy to implement. The ony hindrance was the occasional misclassification confusing the Tello, causing it to align with that &ldquo;face&rdquo;. You can see from these face detection frames that a bounding box was computed. From here, the distance to the target can be inferred from the area of the bounding box and the alignment offset can be inferred from the bounding box center&rsquo;s distance from the frame&rsquo;s center.
vSLAM Fig 5. Example of a map initialization feature match. Typically, the map initialized and I could get a sense of where the features were. Fig 6. Examples of good (left) and average (right) feature extraction. Often times, the good initial feature extractions really set the momentum for how the rest of the main loop would turn out. Notice that the busier nearby area with more edges acquires more features. Fig 7. Examples of map plots and estimated trajectories and camera pose. Both of the movement sequences were left and right images and that the number on the camera indicates that there were 10 keyframes in this vSLAM run. vSLAM was a much harder task to get right. One crux of the system was the speed at which the Tello captured frames; for vSLAM to work well, frames need to be captured in quick succession, with very slight movements. Precisely moving the Tello proved to be very challenging with the MATLAB toolkit, plus an indoor environment where the Tello&rsquo;s own gusts reflecting off of hard surfaces would significantly alter its course. Regardless, the system was still able to generate a point cloud and update location within the map.
Future Streamline main.m with user input to guide the program and improve the functionality of vslam.m as best I can for Tello. Implement general object detection alongside the face detection pipeline. Add autonomous movement based on point cloud &ndash;&gt; remove need for a predetermined path. The implementations here are stepping stones to some more intelligent autonomous UAV behavior. I have the idea that I&rsquo;ll implemennt path planning on a Tello as well. Once I have that, I may integrate these three features into a Tello hide-n-seek project. References Papers P. Viola and M. Jones, “Rapid Object Detection using a Boosted Cascade of Simple Features,” 2001 Comput. Vis. Pattern Recognit., 2001.
E. Rublee, V. Rabaud, K. Konolige, and G. Bradski, “ORB: An efficient alternative to SIFT or SURF,” Proc. IEEE Int. Conf. Comput. Vis., pp. 2564–2571, 2011, doi: 10.1109/ICCV.2011.6126544.
C. Cadena et al., “Past, present, and future of simultaneous localization and mapping: Toward the robust-perception age,” IEEE Trans. Robot., vol. 32, no. 6, pp. 1309–1332, 2016, doi: 10.1109/TRO.2016.2624754.
R. Mur-Artal, J. M. M. Montiel, and J. D. Tardos, “ORB-SLAM: A Versatile and Accurate Monocular SLAM System,” IEEE Trans. Robot., vol. 31, no. 5, pp. 1147–1163, 2015, doi: 10.1109/TRO.2015.2463671.
B. Williams and I. Reid, “On combining visual SLAM and visual odometry,” Proc. - IEEE Int. Conf. Robot. Autom., pp. 3494–3500, 2010, doi: 10.1109/ROBOT.2010.5509248.
Code The vslam.m code is modified from the vSLAM Matlab example. References from my first exposure to quad programming and face detection:
TelloTV TelloPython ]]></content:encoded>
    </item>
    <item>
      <title>dimensionality reduction on neural data</title>
      <link>/notes/neural-dim/</link>
      <pubDate>Mon, 28 Jun 2021 01:58:15 UT</pubDate>
      <dc:creator>Zach Stoebner</dc:creator>
      <guid>/notes/neural-dim/</guid>
      <description>I fell in love with dimensionality reduction when I was learning statistical ML. Since I also study neuroscience, I wanted to practice the art at the intersection of my interests. I compared the 3D projections of a 53-dimensional neurophysiology dataset produced by PCA and a shallow autoencoder.</description>
      <category domain="/categories/course">Course</category>
      <category domain="/categories/solo">Solo</category>
      <content:encoded><![CDATA[ A shallow autoencoder's projection of 53-dimensional vectors to 3 dimensions. tl;dr I fell in love with dimensionality reduction when I was learning statistical ML. Since I also study neuroscience, I wanted to practice the art at the intersection of my interests. I compared the 3D projections of a 53-dimensional neurophysiology dataset produced by PCA and a shallow autoencoder.
Links repo
report
Motivation As I began learning about ML and statistical ML in particular, I became fascinated by dimensionality reduction (DR) methods. For those that don&rsquo;t know, DRs project data from a high-dimensional space to a low-dimensional space. In essence, they are generalizations of the vector projection methods onto the x-, y-, and z-axes taught in a multivariable calculus course. DR is akin to conventional information compression, trading off size for information loss so choosing the best method and lower dimension is as much art as it is strategy.
Content I used this project to put fingers to keyboard and learn through implementation. I explored two avenues, applying
PCA An autoencoder to a waveform-to-cell type classification problem.
PCA is the OG DR method. It decomposes the covariance matrix of the dataset to discover components that explain the most variance of the dataset. Then, the dataset is projected onto these components, which often times could .
Autoencoders (AEs) are a deep neural network (DNN) that learns to encode examples in a dataset to a lower dimensional latent vector and then decode the latent vector back to the original example. Usually, AEs learn to project examples to a manifold, i.e., they are non-linear DR methods.
Essentially, this project compares linear vs. non-linear DR.
Method Dataset The dataset and article Sofroniew, Nicholas James et al. “Neural coding in barrel cortex during whisker-guided locomotion.” can be found on the author&rsquo;s GitHub repo. Of the 16,000 recorded neurons, approx. 30 neurons were recorded for each of 13 subjects. Each recording was comprised of 53 voltage measurements. Overall, the dataset is composed of 302 waveforms. Unavoidably, the dataset is unbalanced; regular spikers comprise 247 of the examples while intermediate spikers only make up 4 examples.
Figure 1. Summary of the waveforms with mean waveform (left) and waveform distributions by cell type (right). Note that the mean waveform is essentially the tightly bounded waveform distribution for regular spikers, which dominate the dataset. Classification To compare and contrast the baseline, PCA, and autoencoding, I implemented a KNN classifier that uses Euclidean distance and majority vote for classification. To find the best number of neighbors given the dataset, I ran it through a standard hyperparameter search using cross-validation and a stratified split of the dataset to mitigate unbalanced classes. Once a good k-value was found, I evaluated the model on the test set, as well as a reclassification of the training set for debugging purposes.
Results The experiments for PCA and autoencoding had the same structure: 1. find the best reduced dimensionality, 2. reduce the dataset, and 3. test with KNN.
Figure 2. Scree plot (left) and cumulative explained variance of the first N components (right) from PCA applied to the waveforms. Table 1. Baseline results for a KNN fit on 53-dimensional waveform feature vectors. Table 2. PCA results for a KNN fit on 3- dimensional waveform components. Figure 3. 3D spatial distribution of the waveform principal components from PCA. Table 3. Autoencoding results for a KNN fit on 3-dimensional waveform components. Figure 4. 3D spatial distribution of the encodings from the bottleneck layer of the autoencoder. For both PCA and autoencoding, the accuracy is only slightly worse than that of the baseline. For PCA, the test accuracy is exactly the same for the 3 seeds while for the autoencoder it is only slightly worse. On the other hand, for the debug accuracy, PCA performs worse than the baseline while the autoencoder performs better. Given that trend, it might suggest that the autoencoder is somewhat overfitting the dataset, diminishing its generalizability. However, the test accuracy suggests that it is not significantly detrimental. All in all, dimensionality reduction still yields data suitable for high performance, even with information loss.
Future PCA is a fixed method but AEs are newer and more flexible. A whole study could be done just exploring AE architectures that yield the best projection for this classification task, not to mention other relevant tasks. Of course, other non-DNN non-linear DR methods could be applied to this dataset, which would be particularly interesting for the classification of waveform to subject. Perhaps one of those methods or an AE would be able to adequately separate these classes, which were not easily separable by PCA when I tried.
Figure 5. 3D spatial distribution of the waveform principal components from PCA for each subject. PCA could not separate these overlapping classes very well,. References Cunningham, J., Yu, B. Dimensionality reduction for large-scale neural recordings. Nat Neurosci 17, 1500–1509 (2014). https://doi.org/10.1038/nn.3776
Paninski L, Cunningham JP. Neural data science: accelerating the experiment-analysis- theory cycle in large-scale neuroscience. Curr Opin Neurobiol. 2018 Jun;50:232-241. doi: 10.1016/j.conb.2018.04.007. PMID: 29738986.
Wu, Tong et al. “Deep Compressive Autoencoder for Action Potential Compression in Large-Scale Neural Recording.” Journal of Neural Engineering 15.6 (2018): n. pag. Journal of Neural Engineering. Web.
Ladjal, Saïd, Alasdair Newson, and Chi Hieu Pham. “A PCA-like Autoencoder.” arXiv 2 Apr. 2019: n. pag. Print.
Scree and cumulative explained variance plots
Matplotlib 3D scatter plot
Keras autoencoder guide
Hyperparameter grid search for Keras:
https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/ https://stackoverflow.com/questions/49823192/autoencoder-gridsearch-hyperparameter-tuning-keras https://towardsdatascience.com/autoencoders-for-the-compression-of-stock-market-data-28e8c1a2da3e ]]></content:encoded>
    </item>
  </channel>
</rss>
