<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/">
  <channel>
    <docs>https://blogs.law.harvard.edu/tech/rss</docs>
    <title>Cps on Zach Stoebner | ECE PhD @ UT Austin</title>
    <link>/tags/cps/</link>
    <description>Recent content in Cps on Zach Stoebner | ECE PhD @ UT Austin</description>
    <image>
      <title>Cps on Zach Stoebner | ECE PhD @ UT Austin</title>
      <link>/tags/cps/</link>
      <url>https://source.unsplash.com/collection/983219/2000x1322</url>
    </image>
    <ttl>1440</ttl>
    <generator>After Dark 9.2.3 (Hugo 0.148.2)</generator>
    <language>en-US</language>
    <copyright>Copyright &copy; Zachary Stoebner. Licensed under CC-BY-ND-4.0.</copyright>
    <lastBuildDate>Sun, 27 Jul 2025 21:45:31 UT</lastBuildDate>
    <atom:link href="/tags/cps/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>autonomous motion planning for an NVIDIA JetBot</title>
      <link>/notes/jetbot-motion-planning/</link>
      <pubDate>Thu, 09 Dec 2021 01:05:08 UT</pubDate>
      <dc:creator>Zach Stoebner</dc:creator>
      <guid>/notes/jetbot-motion-planning/</guid>
      <description>Built a JetBot + an exploration and novice implementation of motion planning on said JetBot. This computational game theory project marked my first foray into optimization and a glimpse of its power muahahaha. It ain&rsquo;t exquisite but it was heading in the right direction.</description>
      <category domain="/categories/course">Course</category>
      <category domain="/categories/solo">Solo</category>
      <content:encoded><![CDATA[ tl;dr Built a JetBot + an exploration and novice implementation of motion planning on said JetBot. This computational game theory project marked my first foray into optimization and a glimpse of its power muahahaha. It ain&rsquo;t exquisite but it was heading in the right direction.
Links repo
report
Background Although autonomous vehicles are the talk of the town these days, the methodologies behind them are elusive to most. To help yourself, I suggest reading my note on LCPs and then follow that up with section 3.1 of this thesis on LMCPs and the path solver for MCPs.
Path planning in autonomous vehicles is a booming research area with significant developments. Although computer vision and machine learning are often employed to plan motion in autonomous vehicles, computationally solving the optimization problems, that arise from in scenarios of motion planning, through a game theoretic is a lightweight alternative to path solving. In this scenario, the path planning optimization problem is formulated as a nonlinear complementarity problem (NCP) constrained by physics and simple car dynamics, which cannot necessarily be directly and exactly solved. Instead the NCP can be approximated by linear mixed complementarity problems (LMCPs), iteratively computing partial paths that together approximate the solution to the NCP and yield a motion planned trajectory for an autonomous vehicle.
The problem formulation is a non-visual scenario where stationary obstacles are laid out on a grid, in a predetermined fashion, and an optimal path must be computed through these obstacles to some goal point without exceeding bounds. Such paths are often nonlinear and can be closely approximated by solving linear mixed complementarity problems via a pathsolver algorithm. Once the path is determined, the JetBot must then move in a real setting, of which the software representation of the grid space is a projection.
Building the JetBot The JetBot was built following the documentation on the JetBot homepage. For the parts with multiple options: the IMX219-160 listed as the second option for cameras, the M2 card + antennas listed as the first option for wifi, and the 65mm wheels listed as the second option for wheels were used. The total cost was approximately $300. The hardware setup time was approximately twelve hours spread between two days. A significant portion of the time was spent extracting a screw terminal from the motor board that was placed incorrectly. 1 shows the completed JetBot hardware assembly.
Motion Path Solver This notebook contains code for non-visual motion planning – the primary objective of the project. The code relies on an LMCP solver that takes an LMCP formulation in M, q, l, u, x 0 and returns a path of points of z, w, v, t. A pathsolver then iteratively solves LMCPs for Newton points along an overarching path, performing backward linesearch to progress sufficiently down each of these paths towards the predefined goal point.
Solving many LMCPs approximates a nonlinear path, which can be formulated as an NMCP for which the KKT conditions must first be derived. The KKT conditions are formulated symbolically so that KKT function as well as the Jacobian of the KKT can be passed to the pathsolver for sparse JIT evaluation, accelerating runtime. Once the point sequence is acquired, it is passed to a module for JetBot motion planning to attempt to move the JetBot along the equivalent trajectory on a real grid space.
Motion Algorithms To achieve the best motion possible on the JetBot, various motion planning algorithms were implemented: linear approximation, Manhattan (aka wiggle) motion, and proportional / integrative / derivative (PID) control. For some of these algorithms, the arctan2 function is used to compute the angle for turning from one orientation to another. (The full code for the JetBotMotion class is included in the appendix.)
arctan 2(∆y, ∆x)
The approximate relevant specifications measured for the JetBot were:
With two obstacles, sometimes the pathsolver fails if dt is too small =⇒ dt &gt; 0.1
Confirmed that the solved states [x, y, v x , v y ] closely approximate the dynamics of horizontal motion
JetBot moves forward 40cm in 0.75 sec at speed=1
JetBot rotates 360 degrees in 1 sec at speed=1
Results Figure 1. Fully assembled JetBot. The two views show the camera, ports, wheels, and overall build structure of the JetBot. In the background is the soldering iron used during assembly. Figure 2. JetBot movement sequence for { (1, 1), (1, 0), (−1, 0), (−1, −1), (0, 1) } . Blue arrows indicate scene flow. The yellow arrow indicates the point that the JetBot should go to next; the yellow circle indicates the final location. Figure 3. Attempted JetBot linear approximation movement on a single obstacle course. The bottom left pane is the predicted trajectory from the pathsolver. Blue arrows indicate scene flow. The yellow arrow indicates the point that the JetBot should go to next; the yellow circle indicates the final location. Figure 4. Attempted JetBot linear approximation movement on a double obstacle course. The bottom left pane is the predicted trajectory from the pathsolver. Blue arrows indicate scene flow. The yellow arrow indicates the point that the JetBot should go to next; the yellow circle indicates the final location. Figure 5. Attempted JetBot Manhattan movement on a double obstacle course. The bottom left pane is the predicted trajectory from the pathsolver. Blue arrows indicate scene flow. The yellow arrow indicates the point that the JetBot should go to next; the yellow circle indicates the final location. Figure 6. Attempted JetBot PID control movement on a double obstacle course. The middle pane is the predicted trajectory from the pathsolver. Blue arrows indicate scene flow. The yellow arrow indicates the point that the JetBot should go to next; the yellow circle indicates the final location. This method resulted in a more correct, but still erroneous, path realization. References Enzenhofer. &ldquo;Numerical Solution of Mixed Linear Complementarity Problems in Multibody Dynamics with Contact.&rdquo; 2018
Dirkse, Steven &amp; Ferris, Michael. (1995). The path solver: a nommonotone stabilization scheme for mixed complementarity problems. Optimization Methods &amp; Software - OPTIM METHOD SOFTW. 5. 123-156. 10.1080/10556789508805606.
[1] Pepy, R., Lambert, A., and Mounier, H., “Path planning using a dynamic vehicle model,” in [2006 2nd International Conference on Information Communication Technologies], 1, 781–786 (2006). [2] Choset, H., La Civita, M., and Park, J., “Path planning between two points for a robot experiencing local- ization error in known and unknown environments,” (11 1999). [3] Lozano-Perez, T., “A simple motion-planning algorithm for general robot manipulators,” IEEE Journal on Robotics and Automation 3(3), 224–238 (1987). [4] Yonetani, R., Taniai, T., Barekatain, M., Nishimura, M., and Kanezaki, A., “Path planning using neural a* search,” in [International Conference on Machine Learning], 12029–12039, PMLR (2021). [5] Lee, L., Parisotto, E., Chaplot, D. S., Xing, E., and Salakhutdinov, R., “Gated path planning networks,” in [International Conference on Machine Learning], 2947–2955, PMLR (2018). [6] Mansouri, S. S., Kanellakis, C., Fresk, E., Kominiak, D., and Nikolakopoulos, G., “Cooperative coverage path planning for visual inspection,” Control Engineering Practice 74, 118–131 (2018). [7] Dirkse, S. and Ferris, M., “The path solver: A non-monotone stabilization scheme for mixed complementarity problems,” Optimization Methods and Software 5 (12 1993). [8] Andersson, J. A. E., Gillis, J., Horn, G., Rawlings, J. B., and Diehl, M., “CasADi – A software framework for nonlinear optimization and optimal control,” Mathematical Programming Computation (In Press, 2018). [9] Araki, M., “Pid control,” CONTROL SYSTEMS, ROBOTICS, AND AUTOMATION 2.
]]></content:encoded>
    </item>
    <item>
      <title>on linear complementarity problems</title>
      <link>/notes/lcp/</link>
      <pubDate>Wed, 24 Nov 2021 17:06:11 UT</pubDate>
      <dc:creator>Zach Stoebner</dc:creator>
      <guid>/notes/lcp/</guid>
      <description>This Fall 2021, I am taking a course on computational game theory, which insofar is the formulation of various games (e.g. bimatrix, Stackelberg) as mathematical programs and the algorithms that solve them, or approximate solutions. Linear complementarity problems are foundational for computing Nash equilibria of simple games.</description>
      <category domain="/categories/algo">Algo</category>
      <content:encoded><![CDATA[tl;dr This Fall 2021, I am taking a course on computational game theory, which insofar is the formulation of various games (e.g. bimatrix, Stackelberg) as mathematical programs and the algorithms that solve them, or approximate solutions. Linear complementarity problems are foundational for computing Nash equilibria of simple games.
Background Programming A mathematical program is an optimization (min, max) over an objective function and constraints. A linear program (LP) is one where the objective function and the constraints are all linear.
The general LP formulation. Aside: The terms &lsquo;argmin&rsquo; and &lsquo;argmax&rsquo; are special terminology for returning the optimizing value of the argument, instead of the optimal value of the function.
Complementarity A complementarity condition is a special kind of constraint required for solving linear complementarity problems (LCPs), as the name suggests. The non-negative vectors x and y are complements if one or both of the values at corresponding indices are 0.
The definition of complementarity. Complementarity results from a program&rsquo;s transformation into an LCP. Generally, it is not a constraint defined in the programs of the game.
Games &amp; Equilibria Intuitively, games are multiple programs that relate to each other. An equilibrium is a simultaneous joint solution that solves all optimization problems in a game.
In Nash games, it is assumed that the opponent will always play their optimal move, so the player should always play their optimal move. A Nash equilibrium is a best player&rsquo;s best move without deviating from their predefined strategy, i.e., a matrix of costs for each of the player&rsquo;s moves against each of the opponent&rsquo;s moves. Solving Nash games is the same as finding the Nash equilibria.
Aside: John Nash proved that in every finite game all players can arrive at an optimal outcome.
Linear Complementarity Problem An LCP is defined as:
The general form of an LCP. LCPs are important and useful, for both mathematical and computer, programming because they can be analytically and algorithmically solved. Therefore, any programs that can be transformed into an LCP can be solved through the LCP; for Nash games, solutions to the LCP are thus the Nash equilibria.
Karush-Kuhn-Tucker Conditions The Karush-Kuhn-Tucker (KKT) conditions are a set of first-order conditions using a program&rsquo;s objective function and constraints that must be satisfied by any optima. For the general form of a program:
The general form of a mathematical program. The KKT conditions are defined as:
The KKT conditions for a general program. The KKT conditions of the programs in a Nash game can be composed into an LCP. Solutions to the LCP satisfy the KKT conditions and are therefore Nash equilibria and solutions to the game.
For the linear program above, the KKT conditions are:
The KKT conditions for a linear program. which can then be composed into a function of the following form:
The KKT conditions as a function for an LCP. Complementarity conditions can then be enforced over this function of KKT conditions to form an LCP. Voilà!
Other programs, e.g., quadratic programs (QPs), can also be massaged into LCPs by stacking their KKT conditions in this way. Some problems can be massaged into complementarity problems, but not necessarily LCPs. Amusingly, the solutions to those problems are often approximated by solving LCPs.
Lemke&rsquo;s Method Lemke&rsquo;s method is a pivoting algorithm for computationally finding solutions to LCPs. The rearranged equation above can be organized into a tableau of the form:
The tableau to solve the LCP via Lemke's method. You may already see that essentially finding solutions to the LCP is solving a system of equations. However, since the tableau is a wide matrix, the system is underdetermined so there are infinitely many solutions, i.e., they lie in a conic region defined by complementary pairs of columns in the tableau, or no solutions, i.e., if q is not contained in any complementary cone.
For n variables in the z vector, there are necessarily 2n columns in the tableau with the n slack variables for w. Because of the stacked KKT conditions in the function, there are n rows. This suggests that any n of the variables, defined by their corresponding columns, can form a basis B to define the conic region containing a feasible solution. The decomposition of the system into basis and non-basis parts:
The decomposition of an undetermined system into basis and non-basis components. Since B is a basis of linearly independent columns, it can be inverted and the basis variables can technically determined that way. However, these matrices can be enormous in practice and inverting a matrix is very computationally expensive. Thankfully, iteratively pivoting the tableau is one way to accomplish the same goal without ever needing to invert a giant matrix. The high-level steps are:
Use a minimum ratio test to determine the exiting variable from the basis. Row-reduce the tableau along the pivot column that corresponds to the entering variable at the row index of the exiting variable s.t. the pivot column is now one-hot at that row index. Replace the exiting variable with the entering variable at its index in the basis vector. The next entering variable is the complementary variable to the exiting variable, i.e., w_i -&gt; z_i. Stop conditions for the algorithm typically include: the values in the pivot columns are out of bounds, i.e., z_i &lt; 0, or the initial entering variable z_0 leaves the basis. At the end of the algorithm, the values in the column corresponding to q will be the values of the basis variables in the final basis, and their complements will be 0. There are a number of modifications and tricks on top of this basic scaffolding that have been developed to handle variant complementarity problems. For discussion of each variant of Lemke&rsquo;s method and their implementation details, I suggest reading Chapter 2 of the Murty textbook.
Other Complementarity Problems &amp; Applications LCPs are in fact a very specific formulation of complementarity problems and generalizations exist, e.g., nonlinear (NCP), linear mixed (LMCP), and mixed (MCP) complementarity problems, which have looser constraints than an LCP. However, these problems typically cannot be analytically solved like LCPs, but can be approximated by LCPs.
Solving games underlies many practical applications, ranging from hot topics, such as autonomous driving and reinforcement learning, to age-old games, i.e., tag, motion planning, i.e., approximating MCPs with the PATH solver, and physics simulations.
A simulation of a tag game. Converting a QP to an LCP and solving via Lemke's method are the crux of generating trajectories for each agent to optimally play the game. References Murty. &ldquo;LINEAR COMPLEMENTARITY, LINEAR AND NONLINEAR PROGRAMMING.&rdquo; 1997.
Cottle, Pang, Stone. &ldquo;The Linear Complementarity Problem.&rdquo; 2008
Dirkse, Steven &amp; Ferris, Michael. (1995). The path solver: a nommonotone stabilization scheme for mixed complementarity problems. Optimization Methods &amp; Software - OPTIM METHOD SOFTW. 5. 123-156. 10.1080/10556789508805606.
GAMS PATH Solver
Nisan et al. &ldquo;Algorithmic Game Theory.&rdquo; 2007
Enzenhofer. &ldquo;Numerical Solution of Mixed Linear Complementarity Problems in Multibody Dynamics with Contact.&rdquo; 2018
Nocedal &amp; Wright. &ldquo;Numerical Optimization.&rdquo; 2006.
Ralph. &ldquo;Global Convergence of Damped Newton&rsquo;s Method for Nonsmooth Equations via the Path Search.&rdquo; 1990.
]]></content:encoded>
    </item>
    <item>
      <title>verification of a VAE &amp; SegNet using NNV</title>
      <link>/notes/vae-segnet-verif/</link>
      <pubDate>Fri, 09 Jul 2021 18:51:01 UT</pubDate>
      <dc:creator>Zach Stoebner</dc:creator>
      <guid>/notes/vae-segnet-verif/</guid>
      <description>Neural network automated verification of a VAE and SegNet using NNV. Although neural networks are promising, they are easily confused, particularly if the input domain is perturbed. In this project, I demonstrate the robustness of MNIST-trained VAE and SegNet against varying brightness attacks.</description>
      <category domain="/categories/course">Course</category>
      <category domain="/categories/solo">Solo</category>
      <content:encoded><![CDATA[ Lower bound and upper bound attacks on a generated image from a VAE. tl;dr Neural network automated verification of a VAE and SegNet using NNV. Although neural networks are promising, they are easily confused, particularly if the input domain is perturbed. In this project, I demonstrate the robustness of MNIST-trained VAE and SegNet against varying brightness attacks.
Links repo
report
Motivation After many face palms and harrowing all-nighters, I learned that achieving reliable performance, for even a task as simple-sounding as face following, is a true feat worthy of ascension from padawan to knight. Even though teams of engineers can concoct relatively high-performing models, I&rsquo;ve learned from first-hand experience, as many others have, that even at the highest level ML messes up&hellip; a lot.
For computer vision in particular, neural networks [NNs] tend to mess up when images are perturbed in some way, e.g., a brightness attack or a deformation, which may be imperceptible to the human eye. Read about optical adversarial attacks against road sign classifiers and defeating deepfake detectors. Subtle intensity differences between images frequently confounds analysis in image-based learning and applications; for example, I also research methods for harmonization of longitudinal MRI datasets using GANs to correct these minute differences. As I was learning about formal methods and automated verification, I wondered whether NNs can be verified for robustness against attacks and, in turn, also verify the performance of generator networks, such as autoencoders, in generating utile images. As a learning experience, this exploratory project is a classic case of &ldquo;know thy enemy&rdquo;.
Content train_segnet.m: trains a SegNet on MNIST train_VAE.m: trains a VAE on MNIST verify_segnet.m: attempts verification of SegNet given perturbed or unperturbed input verify_VAE.m: attempts verification of VAE given perturbed or unperturbed input Other files are ported or modified from MATLAB&rsquo;s VAE demo.
Method For a light background on some of the set theory behind neural network verification and why this task is particularly challenging, please refer to the report linked at the top of this page.
For the first part of the project, I trained a VAE and SegNet, both with the ELBO loss function, on MNIST. Figures 1 and 2 display real and fake images for each network; the VAE tended to blur the image whereas the SegNet learned the structures suspiciously well, potentially a result of overfitting.
Figure 1. Examples of real inputs (left) and fake outputs (right) for the VAE. The pairs demonstrate examples where the VAE performs well and where it does not, noticeably a ‘9’ that becomes an ‘8’ and a ‘4’ that becomes a ‘9’. Figure 2. Examples of real inputs (left) and fake outputs (right) for the SegNet autoencoder. Compared to the VAE, the SegNet is less confused about the structure of the input images. However, the two channel output results in an average of the binary masks which gives a gray image. After training the generator networks, I parsed them into NNV format and appended an MNIST classifier to each of them. In a tutorial, NNV constructed a simple CNN MNIST classifier that performed exceedingly well so I ported that model into this project.
To verify robustness of the generator + classifier, the top 1% or 5% of pixels in input images were eliminated, where the lower bound zeroed out the original intensity while the upper bound kept 5% of the original intensity. From here, NNV constructs polytope set representations of the input domain to feed to the NNV representations of the networks, in order to approximate the entirety of the output domain as a zonotope, star, and abstract polytope reach set.
In terms of analysis, NNV plots error bars, denoting the upper and lower bound probabilities that a given example belongs to each class, for each reach set. A network is robust for a certain class, for a certain set, if the lower bound for the class is greater than the upper bound for all other classes, given an approximated reach set.
Results The approximated zonotope reach set was inconclusive for both models at each level of attack whereas ImageStar and abstract polytope sets had almost identical results and differentiated model performance; therefore, these two reach sets were used in subsequent analyses. For the baseline, the output domains became too varied with greater levels of attack that were not reasonably computable on a CPU so no data was gathered for classes at those levels of attack for the baseline. Table 1 contains a summary of the results for each class for MNIST (baseline) and generated VAE &amp; SegNet images. Both models were particularly robust to the digit 3; the star set results are displayed for this class are displayed in Figure 3.
Table 1. Summary of results per class for baseline, VAE, and SegNet images passed to the classifier. - = robust; {0,1,2,3,4,5,6,7,8,9} = not robust, misclassified to digit; ? = not robust, uncertain, overlapping ranges; ~ = no data. Figure 3. Error bars for the digit ‘3’ for each method for the ImageStar set for the 1% attack. The classifier was distinctly robust to this class for the three types of images, for both attacks. Compared to other classes, the error bars for the ‘3’ class are very narrow for an attacked image. Future This field of NN verification is extremely new and methods for verifying networks with activations other than ReLU and sigmoid are not well-defined. I learned about the field of neural network verification from this project and at the time of writing I am satisfied by what I&rsquo;ve learned. Extending the project, formulating a procedure for verifying generator network outputs with automated NN verification would be a useful tool for many an ML practicioner. A procedure with the current SOTA using my generator + classifier setup might be feasible; however, I expect that we will have to wait for the field to progress significantly further.
References X. Huang et al., “A Survey of Safety and Trustworthiness of Deep Neural Networks: Verification, Testing, Adversarial Attack and Defence, and Interpretability ∗,” arXiv, pp. 0–94, 2018.
C. Liu, T. Arnon, C. Lazarus, C. Barrett, and M. J. Kochenderfer, “Algorithms for verifying deep neural networks,” arXiv, pp. 1–126, 2019, doi: 10.1561/2400000035.
K. Simonyan and A. Zisserman, “Very deep convolutional networks for large-scale image recognition,” 3rd Int. Conf. Learn. Represent. ICLR 2015 - Conf. Track Proc., pp. 1–14, 2015.
H. D. Tran et al., “NNV: The Neural Network Verification Tool for Deep Neural Networks and Learning-Enabled Cyber-Physical Systems,” Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics), vol. 12224 LNCS, pp. 3–17, 2020, doi: 10.1007/978-3- 030-53288-8_1.
Y. Lecun, L. Bottou, Y. Bengio, and P. Ha, “LeNet,” Proc. IEEE, no. November, pp. 1–46, 1998.
D. P. Kingma and M. Welling, “Auto-encoding variational bayes,” 2nd Int. Conf. Learn. Represent. ICLR 2014 - Conf. Track Proc., no. Ml, pp. 1–14, 2014.
V. Badrinarayanan, A. Kendall, and R. Cipolla, “SegNet: A Deep Convolutional Encoder-Decoder Architecture for Image Segmentation,” IEEE Trans. Pattern Anal. Mach. Intell., vol. 39, no. 12, pp. 2481–2495, 2017, doi: 10.1109/TPAMI.2016.2644615.
A. A. Alemi, B. Poole, I. Fische, J. V. Dillon, R. A. Saurous, and K. Murphy, “Fixing a broken elbo,” 35th Int. Conf. Mach. Learn. ICML 2018, vol. 1, pp. 245–265, 2018.
H. D. Tran et al., “Star-based reachability analysis of deep neural networks,” Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics), vol. 11800 LNCS, pp. 670–686, 2019, doi: 10.1007/978-3-030-30942-8_39.
]]></content:encoded>
    </item>
    <item>
      <title>face following and vSLAM for a Tello quadcopter</title>
      <link>/notes/tello-slam/</link>
      <pubDate>Fri, 02 Jul 2021 16:46:28 UT</pubDate>
      <dc:creator>Zach Stoebner</dc:creator>
      <guid>/notes/tello-slam/</guid>
      <description>Implementation of face detection / following and vSLAM on a Ryze Tello using its MATLAB toolkit.</description>
      <category domain="/categories/course">Course</category>
      <category domain="/categories/solo">Solo</category>
      <content:encoded><![CDATA[tl;dr Implementation of face detection / following and vSLAM on a Ryze Tello using its MATLAB toolkit.
Links repo
report
Motivation Following my quad build experience, I set the intention to continue working with and learning more about quads. Whereas in that project I focused more on the hardware side of quads, I wanted to focus more on the software side in this one. Specifically, I wanted to program a quad with autonomous functionality. While working on the quad build, I stumbled upon face detection &amp; following and SLAM. Face detection &amp; following is straightforward: use deep learning to draw a bounding box around faces in the image and compute the direction to travel based on the size and offset from the image&rsquo;s center. vSLAM on the other hand is more interesting in my opinion. For those that don&rsquo;t know: simultaneous localization &amp; mapping (SLAM) uses sensor data, i.e., lidar, radar, camera, etc., to create a map and track the location(s) of the agent(s) on the map. This problem is intractable and elegantly implementing it in the field is a unique challenge, often requiring a team with intimate knowledge of the UAV to tailor clever SLAM algorithms to it.
Contents main.m: control flow script to demonstrate each of these on the Tello. follow.m: face detection and following algorithm that returns the movement vector required to center the drone on the detected face, if there is one. vslam.m: implements vSLAM using the drone&rsquo;s pinhole camera given a predetermined movement sequence that should be cycled a handful of times. Method Face Following Algorithm:
Pass the frame to the object detector and retrieve a bounding box location(s) for the detected object. Draw boxes around all of the detected images. Use the closest bounding box’s width and center coordinates to compute the relative axis change as a percentage of the max. Based on some threshold percentage and some minimum movement distance, set the axes distances and return them to be used in a move command. Fig 1. Face following schematic. An image is passed to the cascade object detector. The detector draws a bounding box around the face. The centering vector from the current center to the center of the bounding box is computed. The UAV moves in the direction of the centering vector while maintaining a safe, specified distance. vSLAM This vSLAM implementation breaks down into three key parts: map initialization, tracking and local mapping.
Starting with map initialization, the steps are as follows:
Track the ORB features on the first image to load the pre-points, then track a second image. Match the ORB feature correspondences between the two images. If enough matches are made (100), compute the homography and fundamental matrices so that the correct geometric transform is applied based on which results in the least error for the relative camera pose. If insufficient matches are made, then the loop restarts on a new image. Manually, the loop has a maximum of 5 iterations to find a matched image until an error is thrown. If a match is not made in 5 iterations, it may imply that the Tello has weak connection and low light and needs to be reset. Triangulate the 3D locations of the matched features in the new map. For tracking:
Move the drone according to the modulus of the current move index by the length of the move sequence. If the Tello loses connection and throws an error, loop back to see if connection is regained, changing no indices except a break iteration countdown of 10. Throw the error if the break countdown expires. Extract ORB features from the frame and match with the latest keyframe. If the new frame is not a keyframe, continue the loop. Estimate the camera pose with Perspective-n-Point [10] in order to project the features to the current frames perspective and correct using some bundle adjustments[8]. This step, although esoteric, is important for the fast computation of that ORB-SLAM offers compared to the competition. Determine if the current frame is a key frame given the criteria. If so, the process continues to local mapping. Else, the loop iterates, and the above steps are redone for the next frame. Additionally, this step also speeds up the process; instead of evaluating all of the features in every frame for mapping, only a select few that are substantially different are filtered for usage. The local mapping steps are as follows:
Add the new keyframe to the set. Compare the keyframes features against all the other keyframe features, looking for unmatched points that occur in at least 3 other keyframes. Bundle adjust the pose based on the adjacent keyframes’ poses. Fig 2. Visual ORB-SLAM schematic. The process starts by initializing the map with two initial frames from the camera. During the initialization the UAV jiggles up and down to snapshot slightly different pairs of images with different feature extractions but still with some matches. If the map initializes, then the program proceeds to the main loop where it first tracks the features on a new frame. If the frame is a keyframe, then the new features are updated into the map. If the frame is not a keyframe, then the loop continues. At the start of each loop iteration, the UAV executes the next move in the sequence. Results Face Following Fig 3. Examples of when my face is detected. Looking at the Tello (left) and not looking at the Tello (right). Nonetheless, it still detects my face and doesn’t pick up much noise, even in low light. Fig 4. Example of face misclassification. These misclassifications typically occur when there is no face in view of the camera. Otherwise, they are rare and not noticeable during a face following run. Face following was easy to implement. The ony hindrance was the occasional misclassification confusing the Tello, causing it to align with that &ldquo;face&rdquo;. You can see from these face detection frames that a bounding box was computed. From here, the distance to the target can be inferred from the area of the bounding box and the alignment offset can be inferred from the bounding box center&rsquo;s distance from the frame&rsquo;s center.
vSLAM Fig 5. Example of a map initialization feature match. Typically, the map initialized and I could get a sense of where the features were. Fig 6. Examples of good (left) and average (right) feature extraction. Often times, the good initial feature extractions really set the momentum for how the rest of the main loop would turn out. Notice that the busier nearby area with more edges acquires more features. Fig 7. Examples of map plots and estimated trajectories and camera pose. Both of the movement sequences were left and right images and that the number on the camera indicates that there were 10 keyframes in this vSLAM run. vSLAM was a much harder task to get right. One crux of the system was the speed at which the Tello captured frames; for vSLAM to work well, frames need to be captured in quick succession, with very slight movements. Precisely moving the Tello proved to be very challenging with the MATLAB toolkit, plus an indoor environment where the Tello&rsquo;s own gusts reflecting off of hard surfaces would significantly alter its course. Regardless, the system was still able to generate a point cloud and update location within the map.
Future Streamline main.m with user input to guide the program and improve the functionality of vslam.m as best I can for Tello. Implement general object detection alongside the face detection pipeline. Add autonomous movement based on point cloud &ndash;&gt; remove need for a predetermined path. The implementations here are stepping stones to some more intelligent autonomous UAV behavior. I have the idea that I&rsquo;ll implemennt path planning on a Tello as well. Once I have that, I may integrate these three features into a Tello hide-n-seek project. References Papers P. Viola and M. Jones, “Rapid Object Detection using a Boosted Cascade of Simple Features,” 2001 Comput. Vis. Pattern Recognit., 2001.
E. Rublee, V. Rabaud, K. Konolige, and G. Bradski, “ORB: An efficient alternative to SIFT or SURF,” Proc. IEEE Int. Conf. Comput. Vis., pp. 2564–2571, 2011, doi: 10.1109/ICCV.2011.6126544.
C. Cadena et al., “Past, present, and future of simultaneous localization and mapping: Toward the robust-perception age,” IEEE Trans. Robot., vol. 32, no. 6, pp. 1309–1332, 2016, doi: 10.1109/TRO.2016.2624754.
R. Mur-Artal, J. M. M. Montiel, and J. D. Tardos, “ORB-SLAM: A Versatile and Accurate Monocular SLAM System,” IEEE Trans. Robot., vol. 31, no. 5, pp. 1147–1163, 2015, doi: 10.1109/TRO.2015.2463671.
B. Williams and I. Reid, “On combining visual SLAM and visual odometry,” Proc. - IEEE Int. Conf. Robot. Autom., pp. 3494–3500, 2010, doi: 10.1109/ROBOT.2010.5509248.
Code The vslam.m code is modified from the vSLAM Matlab example. References from my first exposure to quad programming and face detection:
TelloTV TelloPython ]]></content:encoded>
    </item>
    <item>
      <title>learning about quadcopters by building one</title>
      <link>/notes/quad-build/</link>
      <pubDate>Thu, 01 Jul 2021 21:57:10 UT</pubDate>
      <dc:creator>Zach Stoebner</dc:creator>
      <guid>/notes/quad-build/</guid>
      <description>My introduction to quadcopters and robotics in which I learned a lot about the quad ecosystem and their constitution. However, I suggest following a smaller, cheaper, and more recent build guide. Although my naivete showed through here, my fascination with quads has not soured; check out my Tello face following and vSLAM project!</description>
      <category domain="/categories/personal">Personal</category>
      <content:encoded><![CDATA[ My fully built quadcopter on the ground, not flying... yet. tl;dr My introduction to quadcopters and robotics in which I learned a lot about the quad ecosystem and their constitution. However, I suggest following a smaller, cheaper, and more recent build guide. Although my naivete showed through here, my fascination with quads has not soured; check out my Tello face following and vSLAM project!
Motivation As much as I like software, I also like hardware. I quickly realized that pure computer science wasn&rsquo;t going to expose me to much hardware so I took it upon myself in summer 2020 &ndash; mid-quarantine &ndash; to teach myself. At the time, I was stumbling down a rabbit hole and obsessing over quadcopters, yet I had never laid my hands on one. To me at the time, building a quad from parts was as good a place to start as any.
Cost To make the journey as painless as possible, I followed a build from a $15 Udemy course that was apparently not a moneymaker because it is no longer listed. The build that I followed was not current; the drone industry moves fast and parts will shift in and out of compatibility. You can find potentially better, much cheaper, sufficiently thorough builds on YouTube. The community is dedicated so up-to-date build guides are very likely.
If you don&rsquo;t need tools, then this build is approx $500. Again, there are budget drone builds, that are current and probably more satisfying -- don't reinvent the wheel. Or, you could buy a $100 Tello and program the hell out of it.
Part List Disclaimer: some of these parts may no longer be available.
Raspberry Pi: https://amzn.to/2mrd72g
NAVIO Kit (Need Power module, wires and GPS): https://store.emlid.com/product/navio2/?wpam_id=3
ESCs 4 PACK: https://amzn.to/2kTweBt
Motors 4 PACK: https://amzn.to/2ltKilA
RC Controller: https://amzn.to/2n05Zdq
Frame: https://amzn.to/2mSmNCW
Props: https://amzn.to/2my3w9C
Battery: https://amzn.to/2kSlzHe
Battery Charger: https://amzn.to/2kXA1hi
Telemetry: https://amzn.to/2myfH6l
LiPo Fire-proof Case: https://amzn.to/2lsRu1i
PPM Encoder: https://amzn.to/2n1hjWR
Micro SD Card: https://amzn.to/2lvcJiS
Micro SD to USB: https://amzn.to/2n09yQQ
Battery Connector: https://amzn.to/2ltOP7A or https://amzn.to/2n0a3KI
GPS Mount: https://amzn.to/2luGOiz
Velcro Straps: https://amzn.to/2lsloTe
Scotch Mounting Tape: https://amzn.to/2mSsdxM
Zip Ties: https://amzn.to/2lveUTA
Additional tools if needed:
Soldering Iron Kit: https://amzn.to/2kZklKw
Helping Hands: https://amzn.to/2lvfxwq
Allen Wrench: https://amzn.to/2mVFfus
Electrical Tape: https://amzn.to/2ls4Niv
Spacer Kit: https://amzn.to/2NTIyeY
Drill and Drill Bits: https://amzn.to/2SQrZBK
Lessons Building a drone is easy, getting it to fly is hard. Soldering, plugging, and fixing hardware to the frame is like legos. Flashing the flight OS onto the Raspberry Pi + Navio2 flight controller is no problem. Finding a functioning ground control software from this century = yikes. Then you just have to pray that it&rsquo;s compatible with your flight controller.
On a side note, this drone is deceptively big. Although I&rsquo;m not an expert aerospace engineer, I deduce that getting big things to fly is much more of a hassle than for small things, especially if you live in an apartment in the city.
One super fun takeaway: learning how components connect to the whole quad, how they communicate with each other, and how a full-fledged cyberphysical system can come together was extremely rewarding.
Large quad with exposed wires and point ends = the ultimate cat chew-toy
Future Off and on, I may try to get this quad flying. However, it will likely remain a trophy of my first foray into quads, and also a testament to my initial learning curve. As I&rsquo;ve hinted at many times, I soon intend to follow a smaller, cheaper build. Most importantly, I&rsquo;m excited to take what I learned here and apply it broadly to other hardware projects.
References Caleb Bergquist, instructor of the long lost Udemy course Joshua Bardwell https://ardupilot.org/copter/docs/common-navio2-overview.html ]]></content:encoded>
    </item>
  </channel>
</rss>
