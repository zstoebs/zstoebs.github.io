<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/">
  <channel>
    <docs>https://blogs.law.harvard.edu/tech/rss</docs>
    <title>projects on Zach Stoebner</title>
    <link>/projects/</link>
    <description>Recent content in projects on Zach Stoebner</description>
    <image>
      <title>projects on Zach Stoebner</title>
      <link>/projects/</link>
      <url>https://source.unsplash.com/collection/983219/2000x1322</url>
    </image>
    <ttl>1440</ttl>
    <generator>After Dark 9.2.3 (Hugo 0.81.0)</generator>
    <language>en-US</language>
    <copyright>Copyright &amp;copy; Zachary Stoebner. Licensed under CC-BY-ND-4.0.</copyright>
    <lastBuildDate>Sun, 18 Jul 2021 06:15:09 UT</lastBuildDate>
    <atom:link href="/projects/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>cortical surface analysis for Huntington&#39;s disease using linear-mixed models</title>
      <link>/projects/cortical-surface-analysis/</link>
      <pubDate>Wed, 14 Jul 2021 15:38:19 UT</pubDate>
      <dc:creator>Zach Stoebner</dc:creator>
      <guid>/projects/cortical-surface-analysis/</guid>
      <description>My first completed research project and incredibly my first first-authorship. I wrote code in R and MATLAB to fit LMMs to the cortical data from T1w MRI of HD patients and then perform statistical analyses on the results using SurfStat and random field theory.</description>
      <category domain="/categories/research">Research</category>
      <content:encoded><![CDATA[tl;dr My first completed research project and incredibly my first first-authorship. I wrote code in R and MATLAB to fit LMMs to the cortical data from T1w MRI of HD patients and then performed statistical analyses on the results using SurfStat and random field theory.
Links We are still in the process of submitting our manuscript. As soon as it&amp;rsquo;s published, I will link it here.
Background My first complete research project, the main takeaway was learning the scientific process in action and, most importantly, learning to work with more experienced researchers. I wrote the code and performed all of the analysis that produced our results. However, I did not develop the awesome acquisition method that generated our LGI data nor the statistical theory behind the analysis. Throughout the project, I have relied heavily on the expertise of my co-authors &amp;ndash; all of whom have PhDs whereas I was an undergrad until recently. This first journey in research has been inspiring and indelible and I am beyond grateful for it!
LMMs
random field theory
LGI acquisition method
Abstract The striatum has traditionally been the focus of Huntington’s disease research due to the resulting primary insult and its central role in motor symptoms. Beyond the striatum, evidence of cortical alterations caused by Huntington’s disease has surfaced; however, findings are not coherent between studies. A number of studies have used cortical thickness as the cortical metric of interest due to its establishment in the role of Huntington’s disease. In this study, we propose a more comprehensive approach to cortical morphology using cortical thickness (CT), sulcal depth (SD), and local gyrification index (LGI). Our results show consistency with prior findings in cortical thickness, including its limitations. Comparing between cortical thickness and local gyrification index underscores the complementary nature of these two measures; cortical thickness detects changes in the sensorimotor and posterior areas while local gyrification index identifies insular differences. Since the local gyrification index and cortical thickness measures detect changes in different regions, the two used in tandem could provide a clinically relevant measure of disease progression. Our findings suggest that differences in insular regions correspond to earlier neurodegeneration and may provide a complementary cortical measure for detection of subtle cortical changes due to Huntington’s disease.
Results These results are only the top-most. for all of the tables, charts, etc., please check out the paper once it is published.
CT  Fig 1. Omnibus test results for CT showing the regions of significant contrast across all patients compared to controls. P-values were adjusted for FWER using random field theory (\alpha=0.01).   SD  Fig 2. Omnibus test results for SD showing the regions of significant contrast across all patients compared to controls. P-values were adjusted for FWER using random field theory (\alpha=0.01).   LGI  Fig 3. Omnibus test results for LGI showing the regions of significant contrast across all patients compared to controls. P-values were adjusted for FWER using random field theory (\alpha=0.01).   Summary   Table 1. Summary of Regions with Significant Changes Per Feature. The percentage of the structure with significant changes are reported, in terms of the number of vertices. Regions are color-coded according to cooccurrence in the three features. Red = regional changes were detected by all three features. Yellow = regional changes were detected by two of the features. Blue = regional changes were detected by one of the features.   References [1] F. O. Walker, “Huntington’s disease,” Lancet, vol. 369, no. 9557, pp. 218–228, 2007, doi: 10.1016/S0140-6736(07)60111-1.
[2] J. D. Long et al., “Genetic Modification of Huntington Disease Acts Early in the Prediagnosis Phase,” Am. J. Hum. Genet., vol. 103, no. 3, pp. 349–357, 2018, doi: 10.1016/j.ajhg.2018.07.017.
[3] J. S. Paulsen et al., “Prediction of manifest Huntington disease with clinical and imaging measures : A 12-year prospective observational study,” vol. 13, no. 12, pp. 1193–1201, 2015, doi: 10.1016/S1474-4422(14)70238-8.Prediction.
[4] M. E. Ehrlich, “Huntington’s Disease and the Striatal Medium Spiny Neuron: Cell-Autonomous and Non-Cell-Autonomous Mechanisms of Disease,” Neurotherapeutics, vol. 9, no. 2, pp. 270–284, 2012, doi: 10.1007/s13311-012-0112-2.
[5] K. Hett, H. Johnson, P. Coupe, J. S. Paulsen, J. D. Long, and I. Oguz, “Tensor-Based Grading: A Novel Patch-Based Grading Approach for the Analysis of Deformation Fields in Huntington’s Disease,” Proc. - Int. Symp. Biomed. Imaging, vol. 2020-April, pp. 1091–1095, 2020, doi: 10.1109/ISBI45749.2020.9098692.
[6] H. Li, H. Zhang, H. Johnson, J. Long, J. Paulsen, and I. Oguz, “Longitudinal subcortical segmentation with deep learning,” in SPIE Medical Imaging 2021: Image Processing, 2021, p. 115960D, doi: https://doi.org/10.1117/12.2582340.
[7] H. Li et al., “Generalizing MRI subcortical segmentation to Neurodegeneration,” in MLCN Workshop, MICCAI, 2020, pp. 139–147, doi: https://doi.org/10.1007/978-3-030-66843-3_14.
[8] H. Li, H. Zhang, H. Johnson, J. Long, J. Paulsen, and I. Oguz, “MRI Subcortical Segmentation In Neurodegeneration with Cascaded 3D CNNs,” in SPIE Medical Imaging 2021: Image Processing, 2021, p. 115960W, doi: https://doi.org/10.1117/12.2582005.
[9] J. S. Paulsen et al., “Striatal and white matter predictors of estimated diagnosis for Huntington disease,” Brain Res. Bull., vol. 82, no. 3–4, pp. 201–207, 2010, doi: 10.1016/j.brainresbull.2010.04.003.
[10] J. C. Hedreen, C. E. Peyser, S. E. Folstein, and C. A. Ross, “Neuronal loss in layers V and VI of cerebral cortex in Huntington’s disease,” Neurosci. Lett., vol. 133, no. 2, pp. 257–261, 1991, doi: 10.1016/0304-3940(91)90583-F.
[11] H. D. Rosas et al., “Regional and progressive thinning of the cortical ribbon in Huntington’s disease,” Neurology, vol. 58, no. 5, pp. 695–701, 2002, doi: 10.1212/WNL.58.5.695.
[12] P. C. Nopoulos et al., “Cerebral cortex structure in prodromal Huntington disease,” Neurobiol. Dis., vol. 40, no. 3, pp. 544–554, 2010, doi: 10.1016/j.nbd.2010.07.014.
[13] S. J. Tabrizi et al., “Biological and clinical manifestations of Huntington’s disease in the longitudinal TRACK-HD study: cross-sectional analysis of baseline data Sarah,” vol. 8, no. 9, pp. 791–801, 2013, doi: 10.1016/S1474-4422(09)70170-X.Biological.
[14] B. Fischl and A. M. Dale, “Measuring the thickness of the human cerebral cortex from magnetic resonance images,” Proc. Natl. Acad. Sci. U. S. A., vol. 97, no. 20, pp. 11050–11055, 2000, doi: 10.1073/pnas.200033797.
[15] I. Lyu, H. Kang, N. D. Woodward, and B. A. Landman, “Sulcal depth-based cortical shape analysis in normal healthy control and schizophrenia groups,” vol. 1057402, no. March 2018, p. 1, 2018, doi: 10.1117/12.2293275.
[16] I. Lyu, S. H. Kim, J. B. Girault, J. H. Gilmore, and M. A. Styner, “A cortical shape-adaptive approach to local gyrification index,” Med. Image Anal., vol. 48, pp. 244–258, 2018, doi: 10.1016/j.media.2018.06.009.
[17] D. Wu, A. V. Faria, L. Younes, C. A. Ross, S. Mori, and M. I. Miller, “Whole-brain segmentation and change-point analysis of anatomical brain mri—application in premanifest huntington’s disease,” J. Vis. Exp., vol. 2018, no. 136, pp. 1–9, 2018, doi: 10.3791/57256.
[18] X. Tan, C. A. Ross, M. I. Miller, and X. Tang, “CHANGEPOINT ANALYSIS OF PUTAMEN AND THALAMUS SUBREGIONS IN PREMANIFEST HUNTINGTON’S DISEASE,” in 2018 IEEE 15th International Symposium on Biomedical Imaging, 2018, no. ISBI, pp. 531–535, doi: 10.1109/ISBI.2018.8363632.
[19] X. Tang et al., “Regional subcortical shape analysis in premanifest Huntington’s disease,” Hum. Brain Mapp., vol. 40, no. 5, pp. 1419–1433, 2019, doi: 10.1002/hbm.24456.
[20] Y. Hong et al., “Genetic load determines atrophy in hand cortico-striatal pathways in presymptomatic Huntington’s disease,” Hum. Brain Mapp., vol. 39, no. 10, pp. 3871–3883, 2018, doi: 10.1002/hbm.24217.
[21] J. S. Paulsen et al., “Detection of Huntington’s disease decades before diagnosis: The Predict-HD study,” J. Neurol. Neurosurg. Psychiatry, vol. 79, no. 8, pp. 874–880, 2008, doi: 10.1136/jnnp.2007.128728.
[22] Y. Zhang, J. D. Long, J. A. Mills, J. H. Warner, W. Lu, and J. S. Paulsen, “Indexing disease progression at study entry with individuals at-risk for Huntington disease,” Am. J. Med. Genet. Part B Neuropsychiatr. Genet., vol. 156, no. 7, pp. 751–763, 2011, doi: 10.1002/ajmg.b.31232.
[23] B. Fischl, “FreeSurfer,” Neuroimage, vol. 62, no. 2, pp. 774–781, 2012, doi: 10.1016/j.neuroimage.2012.01.021.FreeSurfer.
[24] I. Lyu, H. Kang, N. D. Woodward, M. A. Styner, and B. A. Landman, “Hierarchical spherical deformation for cortical surface registration,” Med. Image Anal., vol. 57, pp. 72–88, 2019, doi: 10.1016/j.media.2019.06.013.
[25] P. Parvathaneni et al., “Cortical Surface Parcellation Using Spherical Convolutional Neural Networks,” Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif. Intell. Lect. Notes Bioinformatics), vol. 11766 LNCS, pp. 501–509, 2019, doi: 10.1007/978-3-030-32248-9_56.
[26] A. Klein et al., “Open labels: online feedback for a public resource of manually labeled brain images,” 16th Annu. Meet. Organ. Hum. Brain Mapping., p. 84358, 2010.
[27] T. W. J. Moorhead et al., “Automated computation of the Gyrification Index in prefrontal lobes: Methods and comparison with manual implementation,” Neuroimage, vol. 31, no. 4, pp. 1560–1566, 2006, doi: 10.1016/j.neuroimage.2006.02.025.
[28] M. Roberts, J. Hanaway, and D. K. Morest, Atlas of the Human Brain in Section, 2nd ed. Lea &amp;amp; Febiger, 1970.
[29] I. Lyu, S. H. Kim, N. D. Woodward, M. A. Styner, and B. A. Landman, “TRACE: A Topological Graph Representation for Automatic Sulcal Curve Extraction,” IEEE Trans. Med. Imaging, vol. 37, no. 7, pp. 1653–1663, 2018, doi: 10.1109/TMI.2017.2787589.
[30] A. Alin, “Multicollinearity,” Wiley Interdiscip. Rev. Comput. Stat., vol. 2, no. 3, pp. 370–374, 2010, doi: 10.1002/wics.84.
[31] G. Verbeke and G. Molenberghs, Linear Mixed Models for Longitudinal Data, Print. New York: Spinger, 2000.
[32] K. J. Worsley, M. Andermann, T. Koulis, D. MacDonald, and A. C. Evans, “Detecting changes in nonisotropic images,” Hum. Brain Mapp., vol. 8, no. 2–3, pp. 98–101, 1999, doi: 10.1002/(SICI)1097-0193(1999)8:2/3&amp;lt;98::AID-HBM5&amp;gt;3.0.CO;2-F.
[33] J. E. Taylor and K. J. Worsley, “Detecting sparse signals in random fields, with an application to brain mapping,” J. Am. Stat. Assoc., vol. 102, no. 479, pp. 913–928, 2007, doi: 10.1198/016214507000000815.
[34] D. Bates, M. Mächler, B. M. Bolker, and S. C. Walker, “Fitting linear mixed-effects models using lme4,” J. Stat. Softw., vol. 67, no. 1, 2015, doi: 10.18637/jss.v067.i01.
[35] K. Worsley et al., “SurfStat: A Matlab toolbox for the statistical analysis of univariate and multivariate surface and volumetric data using linear mixed effects models and random field theory,” Neuroimage, vol. 47, p. S102, 2009, doi: 10.1016/s1053-8119(09)70882-1.
[36] X. Han et al., “Reliability of MRI-derived measurements of human cerebral cortical thickness: The effects of field strength, scanner upgrade and manufacturer,” Neuroimage, vol. 32, no. 1, pp. 180–194, 2006, doi: 10.1016/j.neuroimage.2006.02.051.
]]></content:encoded>
    </item>
    <item>
      <title>visualizing temporal graph networks</title>
      <link>/projects/tgn-viz/</link>
      <pubDate>Fri, 09 Jul 2021 18:28:40 UT</pubDate>
      <dc:creator>Zach Stoebner</dc:creator>
      <guid>/projects/tgn-viz/</guid>
      <description>Visualizing the resulting link prediction graph from a temporal graph network on a Wikipedia dataset.</description>
      <category domain="/categories/course">Course</category>
      <content:encoded><![CDATA[tl;dr Visualizing the resulting link prediction graph from a temporal graph network on a Wikipedia dataset.
Usage  Clone the repo. cd viz/ python server.py Open the project notebook in Chrome. Safari will likely not work with the server.  Tips:
 If the view cells aren&amp;rsquo;t rendered, run the server endpoint cells. It&amp;rsquo;s a big, complex viz so it may take a while to compute the full graph to a point where you can interact with the views.  Links Observable
GitHub
Motivation It seems like, in the past year, graph neural networks (GNN) have swept through every research circle and someone has spearheaded a journal club presentation about them. Anyways, my friend, Cole, was investigating a new GNN called a temporal graph network (TGN) for his masters thesis. To kill two birds with one stone, he proposed using a TGN for our course project in visual analytics &amp;amp; ML and put into practice this esoteric method that we&amp;rsquo;d only talked about so far.
For more background information, visit the Observable notebook linked above.
Method Preliminaries &amp;amp; Back end: To generate the graph using ACCRE, we ran the TGN on the Wikipedia dataset from http://snap.stanford.edu/jodie/ on both the basic link prediction task for the probability data and the node classification task for the context data. To preprocess and arrange the graph, we used NetworkX. To serve the data, we used Flask and, to compute t-SNE, we used SciKit-Learn. All backend code was written in Python and can be cloned from https://github.com/zstoebs/tgn.
Front end: TGNVis currently has 3 main views: a full graph view, a subgraph view, and a subgraph scatterplot view. The full graph view consists of a link-node diagram that displays all of the nodes in the validation set. Each link color is scaled logarithmically based on the timestamp of the link occurring, with brighter and more saturated colors representing later timestamps. Link opacity linearly encodes the probability of the link occurring according to our model, with more opaque links signifying a higher predicted probability. Users can pan and zoom around the full graph view as well as brush when holding the Alt key (Windows / Linux) or Command key (macOS) to select a subset of nodes that will populate the subgraph view and the scatterplot. When nodes are brushed in the full graph, they will change colors from black to purple. Brushing in d3.js does not coexist well with most other interactions so it is inherently bugged; we suggest waiting for the force simulation to settle and trying to repeatedly to brush.
The subgraph view consists of nodes that have been brushed on the full graph view, plus all of their 1-hop connections. Once the subgraph is populated, hovering over a node or a link will provide the available information about the node or link being displayed. Once again, users can pan and zoom around the subgraph to inspect specific elements. Upon brushing again in the full graph, the subgraph and scatterplots will repopulate.
The scatterplot view contains the 2-component t-SNE dimensionality reduction view of the context embeddings available for the source and destination nodes of links on the subgraph. In order to retain the most information possible for each link, we provide two subplots that display reduced source and destination embeddings, left to right respectively. We also provide brushing functionality for our scatterplot view on the source node plot. When the plot is brushed, the brushed points will change color to blue, and the corresponding points in the destination scatterplot will also change color to red and lines are drawn to identify the links between selected nodes in the scatterplots. In addition, the subgraph view will update the colors of the selected nodes accordingly with one caveat: if it corresponds to both a source and destination embedding, it will change to purple. Nodes are often involved in multiple link events and therefore have multiple instances in
Views Full Graph   Fig 1. The full-graph view displays the full link prediction graph output. It serves as the highest point for analysis. Users can zoom, pan, and brush over a subgraph for closer inspection.   Subgraph   Fig 2. The subgraph view displays the brushed selection from the full-graph view. Hovering over links and nodes populates their information. For nodes, hovering just displays the node&#39;s ID whereas hovering over the link populates source and target IDs, predicted probability, and the timestamp.   Scatterplot   Fig 3. The scatterplot view displays the 2-dim t-SNE reduction for all source and target nodes in the subgraph view. The view allows brushing over the source nodes to identify their corresponding target nodes and also highlight the selected source and target nodes accordingly in the subgraph.   References F. Scarselli, M. Gori, A. C. Tsoi, M. Hagenbuchner, and G. Monfardini, “The graph neural network model,” IEEE Trans. Neural Networks, vol. 20, no. 1, pp. 61–80, 2009, doi: 10.1109/TNN.2008.2005605.
W. L. Hamilton, “Inductive Representation Learning on Large Graphs,” no. Nips, pp. 1–19, 2017.
E. Rossi, B. Chamberlain, F. Frasca, D. Eynard, F. Monti, and M. Bronstein, “Temporal Graph Networks for Deep Learning on Dynamic Graphs,” in ICML, 2020, pp. 1–16.
R. Ying, D. Bourgeois, J. You, M. Zitnik, and J. Leskovec, “GNNExplainer: Generating explanations for graph neural networks,” arXiv, no. iii, 2019.
Z. Jin, Y. Wang, Q. Wang, Y. Ming, T. Ma, and H. Qu, “GNNVis : A Visual Analytics Approach for Prediction Error Diagnosis of Graph Neural Networks,” vol. XX, no. Xx, pp. 1–14, 2020.
Michael Bostock&amp;rsquo;s Temporal Force-Directed Graph
Scax&amp;rsquo;s Force-Directed Graph with Zooming
]]></content:encoded>
    </item>
    <item>
      <title>face following and vSLAM for a Tello quadcopter</title>
      <link>/projects/tello-slam/</link>
      <pubDate>Fri, 02 Jul 2021 16:46:28 UT</pubDate>
      <dc:creator>Zach Stoebner</dc:creator>
      <guid>/projects/tello-slam/</guid>
      <description>Implementation of face detection / following and vSLAM on a Ryze Tello using its MATLAB toolkit.</description>
      <category domain="/categories/course">Course</category>
      <content:encoded><![CDATA[tl;dr Implementation of face detection / following and vSLAM on a Ryze Tello using its MATLAB toolkit.
Links GitHub
Paper
Motivation Following my quad build experience, I set the intention of continuing to work with and learn more about quads. Whereas in that project I focused more on the hardware side of quads, I wanted to focus more on the software side in this one. Specifically, I wanted to program a quad with autonomous functionality. While working on the quad build, I stumbled upon face detection &amp;amp; following and SLAM. Face detection &amp;amp; following is straightforward: use deep learning to draw a bounding box around faces in the image and compute the direction to travel based on the size and offset from the image&amp;rsquo;s center. vSLAM on the other hand is more interesting in my opinion. For those that don&amp;rsquo;t know: simultaneous localization &amp;amp; mapping (SLAM) uses sensor data, i.e., lidar, radar, camera, etc., to create a map and track the location(s) of the agent(s) on the map. This problem is intractable and elegantly implementing it in the field is a unique challenge, often requiring a team with intimate knowledge of the UAV to tailor clever SLAM algorithms to it.
Contents  main.m: control flow script to demonstrate each of these on the Tello. follow.m: face detection and following algorithm that returns the movement vector required to center the drone on the detected face, if there is one. vslam.m: implements vSLAM using the drone&amp;rsquo;s pinhole camera given a predetermined movement sequence that should be cycled a handful of times.  Method Face Following Algorithm:
 Pass the frame to the object detector and retrieve a bounding box location(s) for the detected object. Draw boxes around all of the detected images. Use the closest bounding box’s width and center coordinates to compute the relative axis change as a percentage of the max. Based on some threshold percentage and some minimum movement distance, set the axes distances and return them to be used in a move command.    Fig 1. Face following schematic. An image is passed to the cascade object detector. The detector draws a bounding box around the face. The centering vector from the current center to the center of the bounding box is computed. The UAV moves in the direction of the centering vector while maintaining a safe, specified distance.  vSLAM This vSLAM implementation breaks down into three key parts: map initialization, tracking and local mapping.
Starting with map initialization, the steps are as follows:
 Track the ORB features on the first image to load the pre-points, then track a second image. Match the ORB feature correspondences between the two images. If enough matches are made (100), compute the homography and fundamental matrices so that the correct geometric transform is applied based on which results in the least error for the relative camera pose. If insufficient matches are made, then the loop restarts on a new image. Manually, the loop has a maximum of 5 iterations to find a matched image until an error is thrown. If a match is not made in 5 iterations, it may imply that the Tello has weak connection and low light and needs to be reset. Triangulate the 3D locations of the matched features in the new map.  For tracking:
 Move the drone according to the modulus of the current move index by the length of the move sequence. If the Tello loses connection and throws an error, loop back to see if connection is regained, changing no indices except a break iteration countdown of 10. Throw the error if the break countdown expires. Extract ORB features from the frame and match with the latest keyframe. If the new frame is not a keyframe, continue the loop. Estimate the camera pose with Perspective-n-Point [10] in order to project the features to the current frames perspective and correct using some bundle adjustments[8]. This step, although esoteric, is important for the fast computation of that ORB-SLAM offers compared to the competition. Determine if the current frame is a key frame given the criteria. If so, the process continues to local mapping. Else, the loop iterates, and the above steps are redone for the next frame. Additionally, this step also speeds up the process; instead of evaluating all of the features in every frame for mapping, only a select few that are substantially different are filtered for usage.  The local mapping steps are as follows:
 Add the new keyframe to the set. Compare the keyframes features against all the other keyframe features, looking for unmatched points that occur in at least 3 other keyframes. Bundle adjust the pose based on the adjacent keyframes’ poses.   Fig 2. Visual ORB-SLAM schematic. The process starts by initializing the map with two initial frames from the camera. During the initialization the UAV jiggles up and down to snapshot slightly different pairs of images with different feature extractions but still with some matches. If the map initializes, then the program proceeds to the main loop where it first tracks the features on a new frame. If the frame is a keyframe, then the new features are updated into the map. If the frame is not a keyframe, then the loop continues. At the start of each loop iteration, the UAV executes the next move in the sequence.  Results Face Following   Fig 3. Examples of when my face is detected. Looking at the Tello (left) and not looking at the Tello (right). Nonetheless, it still detects my face and doesn’t pick up much noise, even in low light.     Fig 4. Example of face misclassification. These misclassifications typically occur when there is no face in view of the camera. Otherwise, they are rare and not noticeable during a face following run.   Face following was easy to implement. The ony hindrance was the occasional misclassification confusing the Tello, causing it to align with that &amp;ldquo;face&amp;rdquo;. You can see from these face detection frames that a bounding box computed. From here, the distance to the target can be inferred from the area of the bounding box and the alignment offset can be inferred from the bounding box center&amp;rsquo;s distance from the frame&amp;rsquo;s center.
vSLAM   Fig 5. Example of a map initialization feature match. Typically, the map initialized and I could get a sense of where the features were.     Fig 6. Examples of good (left) and average (right) feature extraction. Often times, the good initial feature extractions really set the momentum for how the rest of the main loop would turn out. Notice that the busier nearby area with more edges acquires more features.     Fig 7. Examples of map plots and estimated trajectories and camera pose. Both of the movement sequences were left and right images and that the number on the camera indicates that there were 10 keyframes in this vSLAM run.  vSLAM was a much harder task to get right. One crux of the system was the speed at which the Tello captured frames; for vSLAM to work well, frames need to be captured in quick succession, with very slight movements. Precisely moving the Tello proved to be very challenging with the MATLAB toolkit, plus an indoor environment where the Tello&amp;rsquo;s own gusts from its propellers reflected off of hard surfaces would significantly alter its course. Regardless, the system was still able to generate a point cloud and update location within the map.
Future  Streamline main.m with user input to guide the program and improve the functionality of vslam.m as best I can for Tello. Implement general object detection alongside the face detection pipeline. Add autonomous movement based on point cloud &amp;ndash;&amp;gt; remove need for a predetermined path. The implementations here are stepping stones to some more intelligent autonomous UAV behavior. I have the idea that I&amp;rsquo;ll implemennt path planning on a Tello as well. Once I have that, I may integrate these three features into a Tello hide-n-seek project.  References Papers P. Viola and M. Jones, “Rapid Object Detection using a Boosted Cascade of Simple Features,” 2001 Comput. Vis. Pattern Recognit., 2001.
E. Rublee, V. Rabaud, K. Konolige, and G. Bradski, “ORB: An efficient alternative to SIFT or SURF,” Proc. IEEE Int. Conf. Comput. Vis., pp. 2564–2571, 2011, doi: 10.1109/ICCV.2011.6126544.
C. Cadena et al., “Past, present, and future of simultaneous localization and mapping: Toward the robust-perception age,” IEEE Trans. Robot., vol. 32, no. 6, pp. 1309–1332, 2016, doi: 10.1109/TRO.2016.2624754.
R. Mur-Artal, J. M. M. Montiel, and J. D. Tardos, “ORB-SLAM: A Versatile and Accurate Monocular SLAM System,” IEEE Trans. Robot., vol. 31, no. 5, pp. 1147–1163, 2015, doi: 10.1109/TRO.2015.2463671.
B. Williams and I. Reid, “On combining visual SLAM and visual odometry,” Proc. - IEEE Int. Conf. Robot. Autom., pp. 3494–3500, 2010, doi: 10.1109/ROBOT.2010.5509248.
Code  The vslam.m code is modified from the vSLAM Matlab example.  References from my first exposure to quad programming and face detection:
 TelloTV TelloPython  ]]></content:encoded>
    </item>
    <item>
      <title>quadcopter build</title>
      <link>/projects/quad-build/</link>
      <pubDate>Thu, 01 Jul 2021 21:57:10 UT</pubDate>
      <dc:creator>Zach Stoebner</dc:creator>
      <guid>/projects/quad-build/</guid>
      <description>A life-changing introduction to quadcopters and robotics in which I learned a lot about the ecosystem and constitution. However, I suggest following a smaller, cheaper, and recent build guide. Although my naivete showed through here, my fascination with quads has not soured; check out my Tello face following and vSLAM project!</description>
      <category domain="/categories/personal">Personal</category>
      <content:encoded><![CDATA[ My fully built quadcopter on the ground, not flying... yet.  tl;dr A life-changing introduction to quadcopters and robotics in which I learned a lot about the ecosystem and constitution. However, I suggest following a smaller, cheaper, and recent build guide. Although my naivete showed through here, my fascination with quads has not soured; check out my Tello face following and vSLAM project!
Motivation As much as I like software, I also like hardware. I quickly realized that pure computer science wasn&amp;rsquo;t going to expose me to much hardware so I took it upon myself in summer 2020 &amp;ndash; mid-quarantine &amp;ndash; to teach myself. At the time, I was stumbling down a rabbit hole and obsessing over quadcopters, yet I had never laid my hands on one. To me at the time, building a quad from parts was as good a place to start as any.
Cost To make the journey as painless as possible, I followed a build from a $15 Udemy course that was apparently not a moneymaker because it is no longer listed. The build that I followed was not current; the drone industry moves fast and parts will shift in and out of compatibility. You can find potentially better, much cheaper, sufficiently thorough builds on YouTube. The community is dedicated so up-to-date build guides are very likely.
If you don&amp;rsquo;t need tools, then this build is approx $500. Again, there are budget drone builds, that are current and probably more satisfying &amp;ndash; don&amp;rsquo;t reinvent the wheel. Or, you could buy a $100 Tello and program the hell out of it.
Part List Disclaimer: some of these parts may no longer be available.
Raspberry Pi: https://amzn.to/2mrd72g
NAVIO Kit (Need Power module, wires and GPS): https://store.emlid.com/product/navio2/?wpam_id=3
ESCs 4 PACK: https://amzn.to/2kTweBt
Motors 4 PACK: https://amzn.to/2ltKilA
RC Controller: https://amzn.to/2n05Zdq
Frame: https://amzn.to/2mSmNCW
Props: https://amzn.to/2my3w9C
Battery: https://amzn.to/2kSlzHe
Battery Charger: https://amzn.to/2kXA1hi
Telemetry: https://amzn.to/2myfH6l
LiPo Fire-proof Case: https://amzn.to/2lsRu1i
PPM Encoder: https://amzn.to/2n1hjWR
Micro SD Card: https://amzn.to/2lvcJiS
Micro SD to USB: https://amzn.to/2n09yQQ
Battery Connector: https://amzn.to/2ltOP7A or https://amzn.to/2n0a3KI
GPS Mount: https://amzn.to/2luGOiz
Velcro Straps: https://amzn.to/2lsloTe
Scotch Mounting Tape: https://amzn.to/2mSsdxM
Zip Ties: https://amzn.to/2lveUTA
Additional tools if needed:
Soldering Iron Kit: https://amzn.to/2kZklKw
Helping Hands: https://amzn.to/2lvfxwq
Allen Wrench: https://amzn.to/2mVFfus
Electrical Tape: https://amzn.to/2ls4Niv
Spacer Kit: https://amzn.to/2NTIyeY
Drill and Drill Bits: https://amzn.to/2SQrZBK
Lessons Building a drone is easy, getting it to fly is hard. Soldering, plugging, and fixing hardware to the frame is like legos. Flashing the flight OS onto the Raspberry Pi &#43; Navio2 flight controller is no problem. Finding a functioning ground control software from this century = yikes. Then you just have to pray that it&amp;rsquo;s compatible with your flight controller.
On a side note, this drone is deceptively big. Although I&amp;rsquo;m not an expert aerospace engineer, I deduce that getting big things to fly is much more of a hassle than for small things, especially if you live in an apartment in the city.
One super fun takeaway: learning how components connect to the whole quad, how they communicate with each other, and how a full-fledged cyberphysical system can come together was extremely rewarding.
Future Off and on, I may try to get this quad flying. However, it will likely remain a trophy of my first foray into quads, and also a testament to my initial learning curve. As I&amp;rsquo;ve hinted at many times, I soon intend to follow a smaller, cheaper build. Most importantly, I&amp;rsquo;m excited to take what I learned here and apply it broadly to other hardware projects.
References  Caleb Bergquist, instructor of the long lost Udemy course Joshua Bardwell https://ardupilot.org/copter/docs/common-navio2-overview.html  ]]></content:encoded>
    </item>
    <item>
      <title>dimensionality reduction on neural data</title>
      <link>/projects/neural-dim/</link>
      <pubDate>Mon, 28 Jun 2021 01:58:15 UT</pubDate>
      <dc:creator>Zach Stoebner</dc:creator>
      <guid>/projects/neural-dim/</guid>
      <description>I fell in love with dimensionality reduction when I was learning statistical ML. Since I also study neuroscience, I wanted to practice the art at the intersection of my interests. I compared the 3D projections of a 53-dimensional neurophysiology dataset produced by PCA and a shallow autoencoder.</description>
      <category domain="/categories/course">Course</category>
      <content:encoded><![CDATA[ A shallow autoencoder&#39;s projection of 53-dimensional vectors to 3 dimensions.  tl;dr I fell in love with dimensionality reduction when I was learning statistical ML. Since I also study neuroscience, I wanted to practice the art at the intersection of my interests. I compared the 3D projections of a 53-dimensional neurophysiology dataset produced by PCA and a shallow autoencoder.
Links GitHub
Paper
Motivation As I began learning about ML and statistical ML in particular, I became fascinated by dimensionality reduction  (DR) methods. For those that don&amp;rsquo;t know, DRs project data from a high-dimensional space to a low-dimensional space. In essence, they are generalizations of the vector projection methods onto the x-, y-, and z-axes taught in a multivariable calculus course. DR is akin to conventional information compression, trading off size for information loss so choosing the best method and lower dimension is as much art as it is strategy.
Content I used this project to put fingers to keyboard and learn through implementation. I explored two avenues, applying
 PCA An autoencoder  to a waveform-to-cell type classification problem.
PCA is the OG DR method. It decomposes the covariance matrix of the dataset to discover components that explain the most variance of the dataset. Then, the dataset is projected onto these components, which often times could .
Autoencoders (AEs) are a deep neural network (DNN) that learns to encode examples in a dataset to a lower dimensional latent vector and then decode the latent vector back to the original example. Usually, AEs learn to project examples to a manifold, i.e., they are non-linear DR methods.
Essentially, this project compares linear vs. non-linear DR.
Method Dataset The dataset and article Sofroniew, Nicholas James et al. “Neural coding in barrel cortex during whisker-guided locomotion.” can be found on the author&amp;rsquo;s GitHub repo. Of the 16,000 recorded neurons, approx. 30 neurons were recorded for each of 13 subjects. Each recording was comprised of 53 voltage measurements. Overall, the dataset is composed of 302 waveforms. Unavoidably, the dataset is unbalanced; regular spikers comprise 247 of the examples while intermediate spikers only make up 4 examples.
 Figure 1. Summary of the waveforms with mean waveform (left) and waveform distributions by cell type (right). Note that the mean waveform is essentially the tightly bounded waveform distribution for regular spikers, which dominate the dataset.  Classification To compare and contrast the baseline, PCA, and autoencoding, I implemented a KNN classifier that uses Euclidean distance and majority vote for classification. To find the best number of neighbors given the dataset, I ran it through a standard hyperparameter search using cross-validation and a stratified split of the dataset to mitigate unbalanced classes. Once a good k-value was found, I evaluated the model on the test set, as well as a reclassification of the training set for debugging purposes.
Results The experiments for PCA and autoencoding had the same structure: 1. find the best reduced dimensionality, 2. reduce the dataset, and 3. test with KNN.
 Figure 2. Scree plot (left) and cumulative explained variance of the first N components (right) from PCA applied to the waveforms.   Table 1. Baseline results for a KNN fit on 53-dimensional waveform feature vectors.   Table 2. PCA results for a KNN fit on 3- dimensional waveform components.   Figure 3. 3D spatial distribution of the waveform principal components from PCA.   Table 3. Autoencoding results for a KNN fit on 3-dimensional waveform components.   Figure 4. 3D spatial distribution of the encodings from the bottleneck layer of the autoencoder.  For both PCA and autoencoding, the accuracy is only slightly worse than that of the baseline. For PCA, the test accuracy is exactly the same for the 3 seeds while for the autoencoder it is only slightly worse. On the other hand, for the debug accuracy, PCA performs worse than the baseline while the autoencoder performs better. Given that trend, it might suggest that the autoencoder is somewhat overfitting the dataset, diminishing its generalizability. However, the test accuracy suggests that it is not significantly detrimental. All in all, dimensionality reduction still yields data suitable for high performance, even with information loss.
Future PCA is a fixed method but AEs are newer and more flexible. A whole study could be done just exploring AE architectures that yield the best projection for this classification task, not to mention other relevant tasks. Of course, other non-DNN non-linear DR methods could be applied to this dataset, which would be particularly interesting for the classification of waveform to subject. Perhaps one of those methods or an AE would be able to adequately separate these classes, which were not easily separable by PCA when I tried.
 Figure 5. 3D spatial distribution of the waveform principal components from PCA for each subject. PCA could not separate these overlapping classes very well,.  References Cunningham, J., Yu, B. Dimensionality reduction for large-scale neural recordings. Nat Neurosci 17, 1500–1509 (2014). https://doi.org/10.1038/nn.3776
Paninski L, Cunningham JP. Neural data science: accelerating the experiment-analysis- theory cycle in large-scale neuroscience. Curr Opin Neurobiol. 2018 Jun;50:232-241. doi: 10.1016/j.conb.2018.04.007. PMID: 29738986.
Wu, Tong et al. “Deep Compressive Autoencoder for Action Potential Compression in Large-Scale Neural Recording.” Journal of Neural Engineering 15.6 (2018): n. pag. Journal of Neural Engineering. Web.
Ladjal, Saïd, Alasdair Newson, and Chi Hieu Pham. “A PCA-like Autoencoder.” arXiv 2 Apr. 2019: n. pag. Print.
Scree and cumulative explained variance plots
Matplotlib 3D scatter plot
Keras autoencoder guide
Hyperparameter grid search for Keras:
 https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/ https://stackoverflow.com/questions/49823192/autoencoder-gridsearch-hyperparameter-tuning-keras https://towardsdatascience.com/autoencoders-for-the-compression-of-stock-market-data-28e8c1a2da3e  ]]></content:encoded>
    </item>
  </channel>
</rss>
