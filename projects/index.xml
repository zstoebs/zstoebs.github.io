<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom" xmlns:content="http://purl.org/rss/1.0/modules/content/" xmlns:dc="http://purl.org/dc/elements/1.1/">
  <channel>
    <docs>https://blogs.law.harvard.edu/tech/rss</docs>
    <title>projects on Zach Stoebner</title>
    <link>/projects/</link>
    <description>Recent content in projects on Zach Stoebner</description>
    <image>
      <title>projects on Zach Stoebner</title>
      <link>/projects/</link>
      <url>https://source.unsplash.com/collection/983219/2000x1322</url>
    </image>
    <ttl>1440</ttl>
    <generator>After Dark 9.2.3 (Hugo 0.81.0)</generator>
    <language>en-US</language>
    <copyright>Copyright &amp;copy; Zachary Stoebner. Licensed under CC-BY-ND-4.0.</copyright>
    <lastBuildDate>Fri, 02 Jul 2021 04:09:47 UT</lastBuildDate>
    <atom:link href="/projects/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Quadcopter Build</title>
      <link>/projects/quad-build/</link>
      <pubDate>Thu, 01 Jul 2021 21:57:10 UT</pubDate>
      <dc:creator>Zach Stoebner</dc:creator>
      <guid>/projects/quad-build/</guid>
      <description>As much as I like software, I also like hardware. I quickly realized that a major in computer science wasn&amp;rsquo;t going to expose to much hardware so I took it upon myself in summer 2020, mid-quarantine, to teach myself hardware. At the time, I stumbled down a rabbit hole of YouTube videos on building quadcopters, yet I had never laid my hands on a quadcopter. Maybe I bit off more than I could chew but I thought that that was as good a place to start as any.</description>
      <category domain="/categories/personal">Personal</category>
      <content:encoded><![CDATA[ My fully built quadcopter.  tl;dr I learned a lot about the ecosystem and consitution of quads. Although my naivete showed through here, my fascination with quads did not sour; check out my Tello face following and vSLAM project. I suggest following a smaller, cheaper, and recent build guide.
Motivation As much as I like software, I also like hardware. I quickly realized that a major in computer science wasn&amp;rsquo;t going to expose me to much hardware so I took it upon myself in summer 2020, mid-quarantine, to teach myself. At the time, I was stumbling down a rabbit hole and obsessing over quadcopters, yet I had never laid my hands on one. Maybe I bit off more than I could chew but I thought that building one from parts was as good a place to start as any.
Cost To make the journey as painless as possible, I followed a build from a $15 Udemy course that was apparently not a moneymaker because it is no longer listed. The build that I followed was not current; tech moves fast and parts will shift in and out of compatibility. You can find much cheaper, sufficiently thorough builds on YouTube and not have to pay to learn. The community is dedicated so up-to-date build guides is very likely.
If you don&amp;rsquo;t need tools, then the drone build is approx $500. Again, there are budget drone builds, that are current and probably more satisfying &amp;ndash; don&amp;rsquo;t reinvent the wheel.
Part List Raspberry Pi: https://amzn.to/2mrd72g
NAVIO Kit (Need Power module, wires and GPS): https://store.emlid.com/product/navio2/?wpam_id=3
ESCs 4 PACK: https://amzn.to/2kTweBt
Motors 4 PACK: https://amzn.to/2ltKilA
RC Controller: https://amzn.to/2n05Zdq
Frame: https://amzn.to/2mSmNCW
Props: https://amzn.to/2my3w9C
Battery: https://amzn.to/2kSlzHe
Battery Charger: https://amzn.to/2kXA1hi
Telemetry: https://amzn.to/2myfH6l
LiPo Fire-proof Case: https://amzn.to/2lsRu1i
PPM Encoder: https://amzn.to/2n1hjWR
Micro SD Card: https://amzn.to/2lvcJiS
Micro SD to USB: https://amzn.to/2n09yQQ
Battery Connector: https://amzn.to/2ltOP7A or https://amzn.to/2n0a3KI
GPS Mount: https://amzn.to/2luGOiz
Velcro Straps: https://amzn.to/2lsloTe
Scotch Mounting Tape: https://amzn.to/2mSsdxM
Zip Ties: https://amzn.to/2lveUTA
Additional tools if needed:
Soldering Iron Kit: https://amzn.to/2kZklKw
Helping Hands: https://amzn.to/2lvfxwq
Allen Wrench: https://amzn.to/2mVFfus
Electrical Tape: https://amzn.to/2ls4Niv
Spacer Kit: https://amzn.to/2NTIyeY
Drill and Drill Bits: https://amzn.to/2SQrZBK
Lessons Building a drone is easy, getting it to fly is hard. Soldering, plugging, and fixing hardware to the frame is like legos. Flashing the flight OS onto the Raspberry Pi and flight controller is no problem. Finding a functioning ground control software (&#43; works with this build) = yikes (&#43; nearly impossible).
On a side note, this drone is deceptively big. Although I&amp;rsquo;m not an expert aerospace engineer, I deduce that getting big things to fly is much more of hassle than for small things, especially if you live in an apartment in the city.
One super fun takeaway: learning how components connect to the whole quad, how they communicate with each other, and how a full-fledged cyberphysical system can come together was extremely rewarding.
Future Off and on, I may try to get this quad flying. However, it will likely remain a trophy of my first foray into quads, and also a testament to my initial learning curve. As I&amp;rsquo;ve hinted at many times, I soon intend to follow a smaller, cheaper build. Most importantly, I&amp;rsquo;m excited to take what I learned here and apply it to other hardware projects.
References   Caleb Bergquist, instructor of the long lost Udemy course
  https://ardupilot.org/copter/docs/common-navio2-overview.html
  ]]></content:encoded>
    </item>
    <item>
      <title>Dimensionality Reduction on Neural Data</title>
      <link>/projects/neural-dim/</link>
      <pubDate>Mon, 28 Jun 2021 01:58:15 UT</pubDate>
      <dc:creator>Zach Stoebner</dc:creator>
      <guid>/projects/neural-dim/</guid>
      <description>As I began learning about ML, I became fascinated by dimensionality reduction methods. For those that don&amp;rsquo;t know, DRs project data from a high-dimensional space to a low-dimensional space. In essence, they are generalizations of the vector projection methods onto the x-, y-, and z-axes taught in a multivariable calculus course. DR is akin to conventional information compression, trading off size for information loss so choosing the best method and lower dimension is as much art as it is strategy.</description>
      <category domain="/categories/course">Course</category>
      <content:encoded><![CDATA[ A shallow autoencoder&#39;s projection of 53-dimensional vectors to 3 dimensions.  GitHub
Motivation As I began learning about ML, I became fascinated by dimensionality reduction  (DR) methods. For those that don&amp;rsquo;t know, DRs project data from a high-dimensional space to a low-dimensional space. In essence, they are generalizations of the vector projection methods onto the x-, y-, and z-axes taught in a multivariable calculus course. DR is akin to conventional information compression, trading off size for information loss so choosing the best method and lower dimension is as much art as it is strategy.
Content I used this project to put fingers to keyboard and learn through implementation. I explored two avenues, applying
 PCA An autoencoder  to a waveform-to-cell type classification problem.
PCA is the OG DR method. It decomposes the covariance matrix of the dataset to discover components that explain the most variance of the dataset. Then, the dataset is projected onto these components, which often times could .
Autoencoders (AEs) are a deep neural network (DNN) that learns to encode examples in a dataset to a lower dimensional latent vector and then decode the latent vector back to the original example. Usually, AEs learn to project examples to a manifold, i.e., they are non-linear DR methods.
Essentially, this project compares linear vs. non-linear DR.
Method Dataset The dataset and article Sofroniew, Nicholas James et al. “Neural coding in barrel cortex during whisker-guided locomotion.” can be found on the author&amp;rsquo;s GitHub repo. Of the 16,000 recorded neurons, approx. 30 neurons were recorded for each of 13 subjects. Each recording was comprised of 53 voltage measurements. Overall, the dataset is composed of 302 waveforms. Unavoidably, the dataset is unbalanced; regular spikers comprise 247 of the examples while intermediate spikers only make up 4 examples.
 Figure 1. Summary of the waveforms with mean waveform (left) and waveform distributions by cell type (right). Note that the mean waveform is essentially the tightly bounded waveform distribution for regular spikers, which dominate the dataset.  Classification To compare and contrast the baseline, PCA, and autoencoding, I implemented a KNN classifier that uses Euclidean distance and majority vote for classification. To find the best number of neighbors given the dataset, I ran it through a standard hyperparameter search using cross-validation and a stratified split of the dataset to mitigate unbalanced classes. Once a good k-value was found, I evaluated the model on the test set, as well as a reclassification of the training set for debugging purposes.
Results The experiments for PCA and autoencoding had the same structure: 1. find the best reduced dimensionality, 2. reduce the dataset, and 3. test with KNN.
 Figure 2. Scree plot (left) and cumulative explained variance of the first N components (right) from PCA applied to the waveforms.   Table 1. Baseline results for a KNN fit on 53-dimensional waveform feature vectors.   Table 2. PCA results for a KNN fit on 3- dimensional waveform components.   Figure 3. 3D spatial distribution of the waveform principal components from PCA.   Table 3. Autoencoding results for a KNN fit on 3-dimensional waveform components.   Figure 4. 3D spatial distribution of the encodings from the bottleneck layer of the autoencoder.  For both PCA and autoencoding, the accuracy is only slightly worse than that of the baseline. For PCA, the test accuracy is exactly the same for the 3 seeds while for the autoencoder it is only slightly worse. On the other hand, for the debug accuracy, PCA performs worse than the baseline while the autoencoder performs better. Given that trend, it might suggest that the autoencoder is somewhat overfitting the dataset, diminishing its generalizability. However, the test accuracy suggests that it is not significantly detrimental. All in all, dimensionality reduction still yields data suitable for high performance, even with information loss.
Future PCA is a fixed method but AEs are newer and more flexible. A whole study could be done just exploring AE architectures that yield the best projection for this classification task, not to mention other relevant tasks. Of course, other non-DNN non-linear DR methods could be applied to this dataset, which would be particularly interesting for the classification of waveform to subject. Perhaps one of those methods or an AE would be able to adequately separate these classes, which were not easily separable by PCA when I tried.
 Figure 5. 3D spatial distribution of the waveform principal components from PCA for each subject. PCA could not separate these overlapping classes very well,.  References Cunningham, J., Yu, B. Dimensionality reduction for large-scale neural recordings. Nat Neurosci 17, 1500–1509 (2014). https://doi.org/10.1038/nn.3776
Paninski L, Cunningham JP. Neural data science: accelerating the experiment-analysis- theory cycle in large-scale neuroscience. Curr Opin Neurobiol. 2018 Jun;50:232-241. doi: 10.1016/j.conb.2018.04.007. PMID: 29738986.
Wu, Tong et al. “Deep Compressive Autoencoder for Action Potential Compression in Large-Scale Neural Recording.” Journal of Neural Engineering 15.6 (2018): n. pag. Journal of Neural Engineering. Web.
Ladjal, Saïd, Alasdair Newson, and Chi Hieu Pham. “A PCA-like Autoencoder.” arXiv 2 Apr. 2019: n. pag. Print.
Scree and cumulative explained variance plots
Matplotlib 3D scatter plot
Keras autoencoder guide
Hyperparameter grid search for Keras:
 https://machinelearningmastery.com/grid-search-hyperparameters-deep-learning-models-python-keras/ https://stackoverflow.com/questions/49823192/autoencoder-gridsearch-hyperparameter-tuning-keras https://towardsdatascience.com/autoencoders-for-the-compression-of-stock-market-data-28e8c1a2da3e  ]]></content:encoded>
    </item>
  </channel>
</rss>
